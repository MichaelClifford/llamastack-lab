{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72af1460-0ba5-499f-baff-123973c362b5",
   "metadata": {},
   "source": [
    "# Llama Stack Core Concepts: Inference Basics\n",
    "\n",
    "Welcome to the foundational steps of working with Llama Stack! In this section, we'll get hands-on with the core capability of Llama Stack: **inference**. This involves sending requests to a Language Model (LLM) and receiving responses.\n",
    "\n",
    "By the end of this module, you will be equipped to:\n",
    "\n",
    "* **Initialize the Llama Stack Client:** Connect your Python code to the Llama Stack server you set up previously.\n",
    "* **Perform Basic Chat Completions:** Send a simple prompt to the LLM and process the standard text response.\n",
    "* **Extract Structured Data:** Learn how to request and parse responses in a defined format (like JSON) for easier programmatic use.\n",
    "\n",
    "Let's start interacting with our Llama Stack server!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089d285-a329-4f61-9edc-3dff39015877",
   "metadata": {},
   "source": [
    "### Install Required Python Libraries\n",
    "\n",
    "Before we write any code interacting with Llama Stack, we need to ensure the necessary Python libraries are installed in our environment.\n",
    "\n",
    "The cell below uses `pip` to install the `llama-stack-client` library (which allows our Python code to communicate with the Llama Stack server) and the `dotenv` library (useful for loading environment variables, though we'll hardcode for simplicity in this lab).\n",
    "\n",
    "**Execute the following code cell** to install the prerequisites using one of the following methods:\n",
    "\n",
    "* Press <kbd>Shift</kbd> + <kbd>Enter</kbd> to run the current cell and move on to the next cell\n",
    "\n",
    "* Press <kbd>Ctrl</kbd> + <kbd>Enter</kbd> to run the cell and stay in the same cell.\n",
    "\n",
    "Run Button: Click the ▶️ \"Run\" button in the toolbar above, it looks like a play button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7268ad4-bda8-427c-9320-3c836e861887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-stack-client==0.2.7 in /home/llama/venv/lib64/python3.12/site-packages (0.2.7)\n",
      "Requirement already satisfied: dotenv in /home/llama/venv/lib64/python3.12/site-packages (0.9.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (4.9.0)\n",
      "Requirement already satisfied: click in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (8.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (0.28.1)\n",
      "Requirement already satisfied: pandas in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (2.2.3)\n",
      "Requirement already satisfied: prompt-toolkit in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (3.0.51)\n",
      "Requirement already satisfied: pyaml in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (25.1.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (2.11.4)\n",
      "Requirement already satisfied: rich in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (14.0.0)\n",
      "Requirement already satisfied: sniffio in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (1.3.1)\n",
      "Requirement already satisfied: termcolor in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/llama/venv/lib64/python3.12/site-packages (from llama-stack-client==0.2.7) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/llama/venv/lib64/python3.12/site-packages (from anyio<5,>=3.5.0->llama-stack-client==0.2.7) (3.10)\n",
      "Requirement already satisfied: certifi in /home/llama/venv/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.2.7) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/llama/venv/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.2.7) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/llama/venv/lib64/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama-stack-client==0.2.7) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/llama/venv/lib64/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.2.7) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/llama/venv/lib64/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.2.7) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/llama/venv/lib64/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.2.7) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv in /home/llama/venv/lib64/python3.12/site-packages (from dotenv) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/llama/venv/lib64/python3.12/site-packages (from pandas->llama-stack-client==0.2.7) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/llama/venv/lib64/python3.12/site-packages (from pandas->llama-stack-client==0.2.7) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/llama/venv/lib64/python3.12/site-packages (from pandas->llama-stack-client==0.2.7) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/llama/venv/lib64/python3.12/site-packages (from pandas->llama-stack-client==0.2.7) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/llama/venv/lib64/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client==0.2.7) (1.17.0)\n",
      "Requirement already satisfied: wcwidth in /home/llama/venv/lib64/python3.12/site-packages (from prompt-toolkit->llama-stack-client==0.2.7) (0.2.13)\n",
      "Requirement already satisfied: PyYAML in /home/llama/venv/lib64/python3.12/site-packages (from pyaml->llama-stack-client==0.2.7) (6.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/llama/venv/lib64/python3.12/site-packages (from rich->llama-stack-client==0.2.7) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/llama/venv/lib64/python3.12/site-packages (from rich->llama-stack-client==0.2.7) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/llama/venv/lib64/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack-client==0.2.7) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-stack-client==0.2.7 dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed2d16-2a83-4f43-90d8-3e87fe66a65f",
   "metadata": {},
   "source": [
    "### Configure Llama Stack Server and Model Endpoints\n",
    "\n",
    "Next, we need to tell our Python script *where* the Llama Stack server is running and *which* specific Language Model we want to use for inference.\n",
    "\n",
    "The code cell below defines Python variables (`LLAMA_STACK_SERVER` and `LLAMA_STACK_MODEL`) that hold the address of your Llama Stack server and the identifier of the model you wish to interact with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a729f5-6954-4ffc-844c-5b758204a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "os.environ['LLAMA_STACK_MODEL'] = 'meta-llama/Llama-3.2-3B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62bc6b-c7a9-40a5-93ff-a0b8dfd75851",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    "> In a production or standard development environment, it's best practice to load sensitive information like server addresses and API keys from environment variables (often stored in a `.env` file) rather than hardcoding them directly in your script. For simplicity in this lab, we are hardcoding these values to make the steps immediately visible and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2008b-85be-4be6-9e05-82cc25aac275",
   "metadata": {},
   "source": [
    "### Initializing the Llama Stack Client\n",
    "\n",
    "The `LlamaStackClient` is your primary interface for communicating with the Llama Stack server. It allows your Python code to access all the capabilities hosted by the server, such as running inference, managing models, and utilizing tools.\n",
    "\n",
    "In this step, we will **initialize the client instance**, providing it with the `base_url` (the server address we defined earlier).\n",
    "\n",
    "This client object is what we will use throughout the lab to send requests and interact with the Llama Stack services. Notice how simple it is to connect – this abstraction is key to Llama Stack's flexibility, allowing you to switch backend infrastructure (like the specific Llama Stack Server instance) with minimal code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "193a3474-d8d9-4ea9-a205-61e5a4b46dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Available models: ---\n",
      "Model Identifier                         Provider ID     Provider Resource ID\n",
      "meta-llama/Llama-3.2-3B-Instruct         ollama          llama3.2:3b-instruct-fp16\n",
      "all-MiniLM-L6-v2                         ollama          all-minilm:latest\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "LLAMA_STACK_MODEL=os.getenv(\"LLAMA_STACK_MODEL\")\n",
    "\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_SERVER)\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print table header\n",
    "print(\"--- Available models: ---\")\n",
    "\n",
    "print(\"Model Identifier                         Provider ID     Provider Resource ID\")\n",
    "\n",
    "for m in models:\n",
    "    print(f\"{m.identifier:40} {m.provider_id:15} {m.provider_resource_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfc89d-a581-4a0d-91dc-849e9849b822",
   "metadata": {},
   "source": [
    "### Performing a Basic Chat Completion\n",
    "\n",
    "With the `LlamaStackClient` initialized, we can now send our first request to the LLM hosted by the server. A common type of interaction is **chat completion**, where we provide a series of messages (representing a conversation) and ask the model to generate the next response.\n",
    "\n",
    "The code cell below demonstrates a basic chat completion request. We provide a simple system message and a user query, then call the `client.inference.chat_completion` method.\n",
    "\n",
    "If you've worked with other LLM frameworks, you'll find this syntax familiar. Llama Stack aims to provide a consistent interface that makes it easy to integrate different models and components. Execute the cell to see the LLM's response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a26696-c9dd-4efd-ae01-a1750e8609a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top speed of a leopard can vary depending on several factors, such as the individual animal's age, sex, and motivation (e.g., chasing prey or escaping from predators). However, according to various sources, including the National Geographic and the World Wildlife Fund, the average running speed of a leopard is around 40-50 km/h (25-31 mph).\n",
      "\n",
      "However, some leopards have been clocked at speeds of up to 60 km/h (37 mph) over short distances, such as when chasing prey or escaping from danger. In fact, one study found that a male leopard can reach speeds of up to 70 km/h (43.5 mph) for brief periods.\n",
      "\n",
      "It's worth noting that leopards are agile and powerful climbers, and they often use their speed and agility to navigate through dense forests and rocky terrain with ease.\n"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=LLAMA_STACK_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the top speed of a leopard?\",\n",
    "        },\n",
    "    ],\n",
    "    # temperature=0.0, \n",
    ")\n",
    "print(response.completion_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91507f-ed66-49a1-8ed5-281b98c1ea7e",
   "metadata": {},
   "source": [
    "### Requesting Structured Data\n",
    "\n",
    "While free-form text responses are useful for conversational AI, often in applications, we need the LLM to return information in a predictable, structured format. This allows our code to easily parse and use the response programmatically.\n",
    "\n",
    "Llama Stack supports requesting responses in formats like JSON based on a defined schema. In this step, we will define a simple data model (`AnimalSpeed` using Pydantic) and ask the LLM to return the animal's speed in a JSON structure matching that model.\n",
    "\n",
    "This is a powerful technique for building reliable integrations with LLMs. **Execute the code cell** and try modifying the user query with different animals to see how the structured output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c7b05a2-8e58-4e83-af0e-9de864580a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Speed:  80\n",
      "Animal:  leopard\n",
      "metric_type:  km/h\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class AnimalSpeed(BaseModel):\n",
    "    speed: int\n",
    "    animal: str\n",
    "    metric_type: str\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=LLAMA_STACK_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the top speed of a leopard?\",            \n",
    "        },\n",
    "    ],\n",
    "    stream=False,    \n",
    "    response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": AnimalSpeed.model_json_schema(),\n",
    "        }\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    response_data = json.loads(response.completion_message.content)\n",
    "    animal = AnimalSpeed(**response_data)    \n",
    "    print(\"-------\")\n",
    "    print(\"Speed: \", animal.speed)\n",
    "    print(\"Animal: \", animal.animal)\n",
    "    print(\"metric_type: \", animal.metric_type)\n",
    "    print(\"-------\")\n",
    "except (json.JSONDecodeError, ValueError) as e:\n",
    "    print(f\"Invalid format: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53906f2a-d632-4bdf-a6b9-60f994809c07",
   "metadata": {},
   "source": [
    "## Module Summary: Llama Stack Inference Basics\n",
    "\n",
    "Great job completing this introductory module! You've successfully taken your first steps in interacting with the Llama Stack server for basic inference tasks.\n",
    "\n",
    "In this section, you've learned how to:\n",
    "\n",
    "* **Establish Connection:** Initialize the `LlamaStackClient` to connect your Python environment to the Llama Stack server.\n",
    "* **Generate Responses:** Send chat completion requests to the LLM and receive free-form text outputs.\n",
    "* **Extract Key Data:** Utilize structured data methods to guide the LLM to return information in a parseable format, like JSON.\n",
    "\n",
    "You now have a fundamental understanding of how to send prompts to an LLM via Llama Stack and process the responses. This forms the bedrock for building more complex applications and agents, which we will explore in the following modules!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
