{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72af1460-0ba5-499f-baff-123973c362b5",
   "metadata": {},
   "source": [
    "# Getting Familiar with Llama Stack Basics\n",
    "\n",
    "In this section of the lab, we will review some basic capabilities of llamastack, by the end of this section, you will be able to: \n",
    "- Initialize the *client* to use the LlamaStack server we created in previous sections\n",
    "- Create a simple *chat completion* request from the llm and recieve a response\n",
    "- Use structured data methods to extact just the information you need from the LLM response "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089d285-a329-4f61-9edc-3dff39015877",
   "metadata": {},
   "source": [
    "### Installing llamatack libraries\n",
    "\n",
    "Let's start by installing the Python Libraries we neeed: (click on the code cell and press Shift + Enter keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7268ad4-bda8-427c-9320-3c836e861887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U llama-stack-client==0.2.2 dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed2d16-2a83-4f43-90d8-3e87fe66a65f",
   "metadata": {},
   "source": [
    "### Define the LLamastack server and Model\n",
    "\n",
    "Let's point our variables to our Llamastack server and chose our desired model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a729f5-6954-4ffc-844c-5b758204a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for our lab, we will just define our variables manualy here, comment the next 2 lines out when using .env in a regular python code scenario\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "os.environ['LLAMA_STACK_MODEL'] = 'meta-llama/Llama-3.2-3B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62bc6b-c7a9-40a5-93ff-a0b8dfd75851",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">When running this code in a regular Python application, we would usually like to read environment variables from an `.env` file, for our needs in this lab, we will hard code these in this cell, to make things more clear\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2008b-85be-4be6-9e05-82cc25aac275",
   "metadata": {},
   "source": [
    "### Initialize the *Client* \n",
    "As a first step, let's define our client, provide it our Llama-Stack Server location and select the model we would like to work with, later, we will see that pointing this to a different location (Llama-Stack Serve) is all we would need to do to move to a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a3474-d8d9-4ea9-a205-61e5a4b46dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "LLAMA_STACK_MODEL=os.getenv(\"LLAMA_STACK_MODEL\")\n",
    "\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_SERVER)\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print table header\n",
    "print(\"--- Available models: ---\")\n",
    "\n",
    "print(\"Model Identifier                         Provider ID     Provider Resource ID\")\n",
    "\n",
    "for m in models:\n",
    "    print(f\"{m.identifier:40} {m.provider_id:15} {m.provider_resource_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfc89d-a581-4a0d-91dc-849e9849b822",
   "metadata": {},
   "source": [
    "### Simple LLM *chat completion* and response\n",
    "Now that our client is set up, let's go through some very simple code snippets, to get you familiar with the syntex. If you used other AI Frameworks, this will soon feel very familiar, as Llamastack follows similar principals and terminology, while allowing a standard to help you quickly shift different components in and out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a26696-c9dd-4efd-ae01-a1750e8609a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=LLAMA_STACK_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the top speed of a leopard?\",\n",
    "        },\n",
    "    ],\n",
    "    # temperature=0.0, \n",
    ")\n",
    "print(response.completion_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91507f-ed66-49a1-8ed5-281b98c1ea7e",
   "metadata": {},
   "source": [
    "### Extracting structued data from response\n",
    "Often, we want the LLM to provide us a specific answer, not in a conversational manner, using structured data can be helpful, later in the lab, you will see how we want certain agents to give us specific facts and not a short story about the facts.\n",
    "\n",
    "Try different animals, to see how the structured data can be helpful for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b05a2-8e58-4e83-af0e-9de864580a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class AnimalSpeed(BaseModel):\n",
    "    speed: int\n",
    "    animal: str\n",
    "    metric_type: str\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=LLAMA_STACK_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the top speed of a leopard?\",            \n",
    "        },\n",
    "    ],\n",
    "    stream=False,    \n",
    "    response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": AnimalSpeed.model_json_schema(),\n",
    "        }\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    response_data = json.loads(response.completion_message.content)\n",
    "    animal = AnimalSpeed(**response_data)    \n",
    "    print(\"-------\")\n",
    "    print(\"Speed: \", animal.speed)\n",
    "    print(\"Animal: \", animal.animal)\n",
    "    print(\"metric_type: \", animal.metric_type)\n",
    "    print(\"-------\")\n",
    "except (json.JSONDecodeError, ValueError) as e:\n",
    "    print(f\"Invalid format: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d677ceb-9135-4d54-91c7-eaeda5825ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
