{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6089d285-a329-4f61-9edc-3dff39015877",
   "metadata": {},
   "source": [
    "Let's start by installing the Python Libraries we neeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efcb5a7",
   "metadata": {},
   "source": [
    "https://github.com/meta-llama/llama-stack/blob/main/docs/notebooks/Llama_Stack_RAG_Lifecycle.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7268ad4-bda8-427c-9320-3c836e861887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -U llama-stack-client dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62bc6b-c7a9-40a5-93ff-a0b8dfd75851",
   "metadata": {},
   "source": [
    "When running this code in a regular Python application, we would usually like to read environment variables from an `.env` file, for our needs in this lab, we will hard code these in this cell, to make things more clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a729f5-6954-4ffc-844c-5b758204a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for our lab, we will just define our variables manualy here:\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8323'\n",
    "os.environ['LLAMA_STACK_MODEL'] = 'meta-llama/Llama-3.2-3B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2008b-85be-4be6-9e05-82cc25aac275",
   "metadata": {},
   "source": [
    "As a first step, let's define our client, provide it our Llama-Stack Server location and select the model we would like to work with, later, we will see that pointing this to a different location (Llama-Stack Serve) is all we would need to do to move to a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a3474-d8d9-4ea9-a205-61e5a4b46dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "LLAMA_STACK_MODEL=os.getenv(\"LLAMA_STACK_MODEL\")\n",
    "\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_SERVER)\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfc89d-a581-4a0d-91dc-849e9849b822",
   "metadata": {},
   "source": [
    "Now that our client is set up, let's go through some very simple code snippets, to get you familiar with the syntex. If you used other AI Frameworks, this will soon feel very familiar, as Llamastack follows similar principals and terminology, while allowing a standard to help you quickly shift different components in and out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91507f-ed66-49a1-8ed5-281b98c1ea7e",
   "metadata": {},
   "source": [
    "Let's see what vectorDBs our server support out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff36145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get provider list and print it out \n",
    "providers = client.providers.list()\n",
    "for provider in providers:\n",
    "    print(provider)\n",
    "    \n",
    "    \n",
    "# select vector_io providers into array\n",
    "vector_providers = [\n",
    "    provider for provider in client.providers.list() if provider.api == \"vector_io\"\n",
    "]\n",
    "\n",
    "# In this example, we only have one provider, but on other server we might have many. here, we simply select the first one.\n",
    "selected_vector_provider = vector_providers[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ed069",
   "metadata": {},
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=selected_vector_provider.provider_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=selected_vector_provider.provider_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca69a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.types import Document\n",
    "urls = [\n",
    "    \"memory_optimizations.rst\",\n",
    "    \"chat.rst\",\n",
    "    \"llama3.rst\",\n",
    "    \"datasets.rst\",\n",
    "    \"qat_finetune.rst\",\n",
    "    \"lora_finetune.rst\",\n",
    "]\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=os.environ['LLAMA_STACK_MODEL'],\n",
    "    instructions=\"You are a helpful assistant that can answer questions about the Torchtune project. You should always use the RAG tool to answer questions.\",\n",
    "    tools=[{\n",
    "        \"name\": \"builtin::rag\",\n",
    "        \"args\": {\"vector_db_ids\": [vector_db_id]},\n",
    "    }],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ce8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's come up with a couple of examples to test the agent\n",
    "examples = [\n",
    "    {\n",
    "        \"input_query\": \"What precision formats does torchtune support?\",\n",
    "        \"expected_answer\": \"Torchtune supports two data types for precision: fp32 (full-precision) which uses 4 bytes per model and optimizer parameter, and bfloat16 (half-precision) which uses 2 bytes per model and optimizer parameter.\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What does DoRA stand for in torchtune?\",\n",
    "        \"expected_answer\": \"Weight-Decomposed Low-Rank Adaptation\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"How does the CPUOffloadOptimizer reduce GPU memory usage?\",\n",
    "        \"expected_answer\": \"The CPUOffloadOptimizer reduces GPU memory usage by keeping optimizer states on CPU and performing optimizer steps on CPU. It can also optionally offload gradients to CPU by using offload_gradients=True\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"How do I ensure only LoRA parameters are trainable when fine-tuning?\",\n",
    "        \"expected_answer\": \"You can set only LoRA parameters to trainable using torchtune's utility functions: first fetch all LoRA parameters with lora_params = get_adapter_params(lora_model), then set them as trainable with set_trainable_params(lora_model, lora_params). The LoRA recipe handles this automatically.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import pprint\n",
    "import rich\n",
    "\n",
    "for example in examples:\n",
    "    rag_session_id = rag_agent.create_session(session_name=f\"rag_session_{uuid.uuid4()}\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example[\"input_query\"]\n",
    "            }\n",
    "        ],\n",
    "        session_id=rag_session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    rich.print(f\"[bold cyan]Question:[/bold cyan] {example['input_query']}\")\n",
    "    rich.print(f\"[bold yellow]Agent Answer:[/bold yellow] {response.output_message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8af31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session_response = client.agents.session.retrieve(agent_id=rag_agent.agent_id, session_id=rag_agent.sessions[1])\n",
    "pprint(session_response.turns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
