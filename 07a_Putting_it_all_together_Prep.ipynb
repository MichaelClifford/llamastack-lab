{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515d4dbb-e76f-4c51-ae48-a490bd6f6590",
   "metadata": {},
   "source": [
    "# Putting it all together - Prepering the environment\n",
    "You would have gone through most of the elements in this part already, but in case you skipped ahead, and to ensure a clean slate, we will set up our environment from scratch, with a few new elements.\n",
    "We are going to go through stuff a bit faster, if anyting is unclear, it's recommended to revisit previous sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c58715-74f5-4c41-af9b-2e08f537746f",
   "metadata": {},
   "source": [
    "### Setting up the environment for our experiment\n",
    "Let's begin by importing necessary utilities for displaying agent steps in a human-readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc7d613-ed4d-43d3-95a5-2f4102b808ef",
   "metadata": {},
   "source": [
    "We will now start our  MCP Servers: \n",
    "* `mcp-weather` - This server will expose weather-related functionalities that our Llama Stack agent can use as tools.\n",
    "* `mcp-googlemaps` - this server will allow us to query the google maps api and provide information about locations, businesses and driving directions\n",
    "* `mcp-parks-info` - This server is a wrapper for our built-in::rag, it will response to sepecific questions about out parks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0686d7-8e05-416e-be18-7d3abe0ecdc0",
   "metadata": {},
   "source": [
    "```bash\n",
    "# MCP-weather configuration\n",
    "REMOTE_REGISTRY=\"quay.dev.demo.redhat.com/rhdp/\"\n",
    "\n",
    "podman run -d --name mcp-weather --network=host ${REMOTE_REGISTRY}mcp-weather:latest --port 8005  \n",
    "\n",
    "# MCP-google-maps configuration\n",
    "\n",
    "YOUR_API_KEY=\"AIzaSyAYcxmnVi7ODNOT_A_REAL_KEY_REPLACE_ME\"\n",
    "podman run -d --name mcp-googlemaps-sse --network=host -e GOOGLE_MAPS_API_KEY=\"ENTER_YOUR_TOKEN\" -e MCP_PORT=8006 ${REMOTE_REGISTRY}mcp-googlemaps-sse:latest\n",
    "\n",
    "# MCP-parks-info\n",
    "\n",
    "podman run -d --name mcp-parks-info --network=host -e PORT=8007 quay.dev.demo.redhat.com/rhdp/mcp-parks-info:latest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7efd52-e1f8-4e2a-b76d-b15e2895b184",
   "metadata": {},
   "source": [
    "Once the MCP server is running, we can quickly verify its availability by attempting to connect to its `/sse` (Server-Sent Events) endpoint. A successful response indicates the server is live.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20113860-d39e-4974-b660-10825790179b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MCP-WEATHER\n",
      "event: endpoint\n",
      "data: /messages/?session_id=b46073e3773e4cd3add304714383e03a\n",
      "\n",
      "Testing MCP-GOOGLEMAPS-sse\n",
      "event: endpoint\n",
      "data: /message?sessionId=2073da1f-ff0b-4b1d-8c49-fe1696dd9141\n",
      "\n",
      "Testing mcp-parks-info\n",
      "event: endpoint\n",
      "data: /messages/?session_id=e45b99af09c04e42b94894fd359e9b32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!echo Testing MCP-WEATHER ; curl --max-time 1 http://localhost:8005/sse 2>/dev/null\n",
    "!echo Testing MCP-GOOGLEMAPS-sse ; curl --max-time 1 http://localhost:8006/sse 2>/dev/null \n",
    "!echo Testing mcp-parks-info ; curl --max-time 1 http://localhost:8007/sse 2>/dev/null \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e9167-1613-45df-9819-f1e7bcc91905",
   "metadata": {},
   "source": [
    "### Setting up Llama Stack Agent (Stuff we already know)\n",
    "Now, let's set up our Llama Stack client. This involves importing necessary libraries, configuring the Llama Stack server URL, and selecting the language model our agent will use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f8d7728-a109-43c2-bb8b-96d8f0c11d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip Python Prerequisites installed succesfuly\n",
      "--- Available models: ---\n",
      "all-MiniLM-L6-v2 - ollama - all-minilm:latest\n",
      "granite3.2:8b - ollama - granite3.2:8b\n",
      "meta-llama/Llama-3.2-3B-Instruct - ollama - llama3.2:3b-instruct-fp16\n",
      "Selected model (from allowed list): meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-stack-client==0.2.5 dotenv > /dev/null 2>&1 && echo \"pip Python Prerequisites installed succesfuly\"\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from src.utils import step_printer\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# These libraries are just here to print the results from the agent in a more human-readable way \n",
    "from termcolor import cprint\n",
    "import uuid\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "stream=False ## Defaulting to False, you can change this throughout the section to \"True\" if you wanted to see the output in another format (Using EventLogger)\n",
    "\n",
    "# for our lab, we will just define our variables manualy here, in a regular application, this would be ready directly from the local .env file and we would comment these lines out\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "\n",
    "# We will be using the Tavily web search service (docs.tavily.com/)\n",
    "tavily_search_api_key='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'\n",
    "provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "\n",
    "# List available models and select from allowed models list\n",
    "allowed_models_list=[\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "#allowed_models_list=[\"granite3.2:8b\"]\n",
    "\n",
    "selected_model = None\n",
    "\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "LLAMA_STACK_SERVER='http://localhost:8321'\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMA_STACK_SERVER,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n",
    "    # Check if the model identifier contains any of the allowed substrings\n",
    "    if any(substring in m.identifier for substring in allowed_models_list):\n",
    "        # Only set selected_model if it hasn't been set yet\n",
    "        if selected_model is None:\n",
    "            selected_model = m.identifier\n",
    "           \n",
    "# If no allowed model was found, you might want to handle that case\n",
    "if selected_model is None:\n",
    "    print(\"No allowed model found in the list.\")\n",
    "\n",
    "\n",
    "print(f\"Selected model (from allowed list): {selected_model}\")\n",
    "            # Removed the break here to show all available models, but the selection logic remains picking the first one\n",
    "\n",
    "\n",
    "SELECTED_MODEL = selected_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9de5c-6f9e-43da-aac3-3b43bc62fb50",
   "metadata": {},
   "source": [
    "### Registering the new MCP Servers as a tools\n",
    "This is a crucial step: we are now registering our MCP Weather servers as a 'toolgroup' with Llama Stack. This makes the functionalities exposed by them available for our agents to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d23404fc-4496-4d65-8f6c-24ead07cb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::mcp-weather\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":\"http://localhost:8005/sse\"},\n",
    "    )\n",
    "\n",
    "client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::mcp-googlemaps\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":\"http://localhost:8006/sse\"},\n",
    "    )\n",
    "client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::mcp-parks-info\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":\"http://localhost:8007/sse\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7811cc-2a4d-4db9-89f2-06a6170ed072",
   "metadata": {},
   "source": [
    "## Creating a VectorDB for our RAG to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca62f196-4166-40d3-a819-3b37cf896fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unregistering vector database: Our_Parks_DB\n",
      "Created 5 documents from local files into Our_Parks_DB\n"
     ]
    }
   ],
   "source": [
    "#Unregistering vector database in case we are running this over and over again as part of a learning experience\n",
    "\n",
    "for vector_db_id in client.vector_dbs.list():\n",
    "    print(f\"Unregistering vector database: {vector_db_id.identifier}\")\n",
    "    client.vector_dbs.unregister(vector_db_id=vector_db_id.identifier)\n",
    "\n",
    "# Select a VectorDB provider from availalbe providers\n",
    "providers = client.providers.list()\n",
    "vector_providers = []\n",
    "for provider in client.providers.list():\n",
    "    if provider.api == \"vector_io\":\n",
    "        #print(f\"Found VectorDB provider: {provider.provider_id}\\n\")  # Simple print\n",
    "        vector_providers.append(provider)\n",
    "\n",
    "# In this example, we only have one provider, but on other server we might have many. here, we simply select the first one.\n",
    "selected_vector_provider = vector_providers[0]\n",
    "\n",
    "# register our DB\n",
    "vector_db_id = \"Our_Parks_DB\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=selected_vector_provider.provider_id,\n",
    ")\n",
    "\n",
    "# Injest documents from local directory (this is so you can test inserting changes easily)\n",
    "from llama_stack_client.types import Document\n",
    "\n",
    "files = [\n",
    "    \"Azure_Mongrove_Wilderness.md\",\n",
    "    \"Crimson_Basin.md\",\n",
    "    \"Granite_Spire.md\",\n",
    "    \"Obsidian_Rainforest.md\",\n",
    "    \"Prismatic_Painted_Prairie.md\",\n",
    "]\n",
    "\n",
    "document_dirctory=\"assets/Parks\"\n",
    "\n",
    "# Read documents into the \"documents\" array\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=open(os.path.join(document_dirctory, file), 'r', encoding='utf-8').read(),\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, file in enumerate(files)\n",
    "]\n",
    "\n",
    "# Insert the documents into the vectorDB\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=300,\n",
    ")\n",
    "\n",
    "print(f\"Created {len(documents)} documents from local files into {vector_db_id}\")\n",
    "\n",
    "\n",
    "#print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656d8a7-e759-4bb2-9fe1-afac153960c4",
   "metadata": {},
   "source": [
    "# Summary \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08368a3-2cfd-47e7-bbf1-f77d550222e1",
   "metadata": {},
   "source": [
    "### Useful tools - Remove MCP Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33be14de-6a8d-41ac-96be-a9f7adb38eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to unregister toolgroup: mcp::mcp-weather\n",
      "Failed to unregister toolgroup mcp::mcp-weather. Error: Error code: 400 - {'detail': \"Invalid value: Tool group 'mcp::mcp-weather' not found\"}\n",
      "Attempting to unregister toolgroup: mcp::mcp-googlemaps\n",
      "Failed to unregister toolgroup mcp::mcp-googlemaps. Error: Error code: 400 - {'detail': \"Invalid value: Tool group 'mcp::mcp-googlemaps' not found\"}\n",
      "Attempting to unregister toolgroup: mcp::mcp-parks-info\n",
      "Failed to unregister toolgroup mcp::mcp-parks-info. Error: Error code: 400 - {'detail': \"Invalid value: Tool group 'mcp::mcp-parks-info' not found\"}\n",
      "\n",
      "Finished attempting to unregister toolgroups.\n"
     ]
    }
   ],
   "source": [
    "toolgroups_to_unregister = [\n",
    "    \"mcp::mcp-weather\",\n",
    "    \"mcp::mcp-googlemaps\",\n",
    "    \"mcp::mcp-parks-info\",\n",
    "]\n",
    "\n",
    "for toolgroup_id in toolgroups_to_unregister:\n",
    "    try:\n",
    "        print(f\"Attempting to unregister toolgroup: {toolgroup_id}\")\n",
    "        client.toolgroups.unregister(toolgroup_id=toolgroup_id)\n",
    "        print(f\"Successfully unregistered toolgroup: {toolgroup_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to unregister toolgroup {toolgroup_id}. Error: {e}\")\n",
    "\n",
    "print(\"\\nFinished attempting to unregister toolgroups.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
