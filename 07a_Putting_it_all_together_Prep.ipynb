{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515d4dbb-e76f-4c51-ae48-a490bd6f6590",
   "metadata": {},
   "source": [
    "# Putting it all together - Preparing the Environment\n",
    "\n",
    "Welcome back! In the previous sections, you've explored the foundational building blocks of developing with Llama Stack: understanding basic agents, delving into the power of ReAct agents, and seeing how the Model Context Protocol (MCP) allows seamless integration of external services as tools.\n",
    "\n",
    "Now, it's time to bring these concepts together and prepare our environment for building a more sophisticated agent capable of interacting with multiple external services and leveraging internal knowledge. This section is dedicated to setting the stage, ensuring all the necessary components are up and running and configured correctly within Llama Stack.\n",
    "\n",
    "By the end of this preparatory section, you will have:\n",
    "\n",
    "* Understood the setup required for a multi-tool, RAG-enabled agent.\n",
    "* Launched and verified the operation of multiple MCP servers exposing diverse functionalities.\n",
    "* Configured your Llama Stack client to interact with these new tools.\n",
    "* Registered the MCP servers as discoverable toolgroups within Llama Stack.\n",
    "* Created and populated a Vector Database, essential for providing our agent with domain-specific knowledge via Retrieval Augmented Generation (RAG).\n",
    "\n",
    "Think of this section as gathering and organizing all the ingredients and setting up your workspace before you start cooking a complex and exciting dish! Let's get everything ready so we can build something truly powerful in the next section.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c58715-74f5-4c41-af9b-2e08f537746f",
   "metadata": {},
   "source": [
    "### Setting up our Environment\n",
    "\n",
    "Before we dive into the exciting part of building our multi-tool agent, let's make sure our environment is set up correctly. We'll start by importing the essential libraries and configuring our connection to the Llama Stack server, just like we did in previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f8d7728-a109-43c2-bb8b-96d8f0c11d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip Python Prerequisites installed succesfuly\n",
      "--- Available models: ---\n",
      "all-MiniLM-L6-v2 - ollama - all-minilm:latest\n",
      "granite3.2:8b - ollama - granite3.2:8b\n",
      "meta-llama/Llama-3.2-3B-Instruct - ollama - llama3.2:3b-instruct-fp16\n",
      "Selected model (from allowed list): meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-stack-client==0.2.5 dotenv > /dev/null 2>&1 && echo \"pip Python Prerequisites installed succesfuly\"\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from src.utils import step_printer\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# These libraries are just here to print the results from the agent in a more human-readable way \n",
    "from termcolor import cprint\n",
    "import uuid\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "stream=False ## Defaulting to False, you can change this throughout the section to \"True\" if you wanted to see the output in another format (Using EventLogger)\n",
    "\n",
    "# for our lab, we will just define our variables manualy here, in a regular application, this would be ready directly from the local .env file and we would comment these lines out\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "\n",
    "# We will be using the Tavily web search service (docs.tavily.com/)\n",
    "tavily_search_api_key='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'\n",
    "provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "\n",
    "# List available models and select from allowed models list\n",
    "allowed_models_list=[\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "#allowed_models_list=[\"granite3.2:8b\"]\n",
    "\n",
    "selected_model = None\n",
    "\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "LLAMA_STACK_SERVER='http://localhost:8321'\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMA_STACK_SERVER,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n",
    "    # Check if the model identifier contains any of the allowed substrings\n",
    "    if any(substring in m.identifier for substring in allowed_models_list):\n",
    "        # Only set selected_model if it hasn't been set yet\n",
    "        if selected_model is None:\n",
    "            selected_model = m.identifier\n",
    "           \n",
    "# If no allowed model was found, you might want to handle that case\n",
    "if selected_model is None:\n",
    "    print(\"No allowed model found in the list.\")\n",
    "\n",
    "\n",
    "print(f\"Selected model (from allowed list): {selected_model}\")\n",
    "            # Removed the break here to show all available models, but the selection logic remains picking the first one\n",
    "\n",
    "\n",
    "SELECTED_MODEL = selected_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc7d613-ed4d-43d3-95a5-2f4102b808ef",
   "metadata": {},
   "source": [
    "### Launching our External Services (MCP Servers)\n",
    "\n",
    "To create a truly capable agent, we need to connect it to the outside world! We'll be using a few specialized external services, exposed via the Model Context Protocol (MCP), to give our agent powers like checking the weather, finding locations, and accessing specific park information.\n",
    "\n",
    "Run the following commands in your terminal to start these essential MCP servers in the background:\n",
    "\n",
    "* `mcp-weather`: Adds real-time weather capabilities.\n",
    "* `mcp-googlemaps`: Connects to Google Maps for location and direction data.\n",
    "* `mcp-parks-info`: Offers RAG-based information about our fictional parks using RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7323e816-10cb-4f2e-a9a9-25001565b4b5",
   "metadata": {},
   "source": [
    "```bash\n",
    "# MCP-weather configuration\n",
    "REMOTE_REGISTRY=\"[quay.dev.demo.redhat.com/rhdp/](https://quay.dev.demo.redhat.com/rhdp/)\"\n",
    "\n",
    "podman run -d --name mcp-weather --network=host ${REMOTE_REGISTRY}mcp-weather:latest --port 8005  \n",
    "\n",
    "# MCP-google-maps configuration\n",
    "\n",
    "YOUR_API_KEY=\"AIzaSyAYcxmnVi7ODNOT_A_REAL_KEY_REPLACE_ME\"\n",
    "podman run -d --name mcp-googlemaps-sse --network=host -e Maps_API_KEY=\"ENTER_YOUR_TOKEN\" -e MCP_PORT=8006 ${REMOTE_REGISTRY}mcp-googlemaps-sse:latest\n",
    "\n",
    "# MCP-parks-info\n",
    "\n",
    "podman run -d --name mcp-parks-info --network=host -e PORT=8007 [quay.dev.demo.redhat.com/rhdp/mcp-parks-info:latest](https://quay.dev.demo.redhat.com/rhdp/mcp-parks-info:latest)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7efd52-e1f8-4e2a-b76d-b15e2895b184",
   "metadata": {},
   "source": [
    "### Verify Servers are Running\n",
    "\n",
    "It's always a good practice to quickly check if our external services have started successfully. We can do this by attempting to connect to their status endpoints. Run the code below to verify they are live and ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20113860-d39e-4974-b660-10825790179b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MCP-WEATHER\n",
      "event: endpoint\n",
      "data: /messages/?session_id=346da1b78ad24cd3a43ba9f329f7fa5a\n",
      "\n",
      "Testing MCP-GOOGLEMAPS-sse\n",
      "event: endpoint\n",
      "data: /message?sessionId=97181020-f683-4cf5-83c1-cacb0d1eb53e\n",
      "\n",
      "Testing mcp-parks-info\n",
      "event: endpoint\n",
      "data: /messages/?session_id=a36ce378f8cc409ba46bddb23458c1aa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!echo Testing MCP-WEATHER ; curl --max-time 1 http://localhost:8005/sse 2>/dev/null\n",
    "!echo Testing MCP-GOOGLEMAPS-sse ; curl --max-time 1 http://localhost:8006/sse 2>/dev/null \n",
    "!echo Testing mcp-parks-info ; curl --max-time 1 http://localhost:8007/sse 2>/dev/null \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e9167-1613-45df-9819-f1e7bcc91905",
   "metadata": {},
   "source": [
    "### Connect to Llama Stack and Select Our Model\n",
    "\n",
    "Now that our external services are running, let's establish the connection from our notebook to the Llama Stack server and select the powerful language model our agent will use for reasoning and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf6a1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip Python Prerequisites installed succesfuly\n",
      "--- Available models: ---\n",
      "all-MiniLM-L6-v2 - ollama - all-minilm:latest\n",
      "granite3.2:8b - ollama - granite3.2:8b\n",
      "meta-llama/Llama-3.2-3B-Instruct - ollama - llama3.2:3b-instruct-fp16\n",
      "Selected model (from allowed list): meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-stack-client==0.2.5 dotenv > /dev/null 2>&1 && echo \"pip Python Prerequisites installed succesfuly\"\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from src.utils import step_printer\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# These libraries are just here to print the results from the agent in a more human-readable way \n",
    "from termcolor import cprint\n",
    "import uuid\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "stream=False ## Defaulting to False, you can change this throughout the section to \"True\" if you wanted to see the output in another format (Using EventLogger)\n",
    "\n",
    "# for our lab, we will just define our variables manualy here, in a regular application, this would be ready directly from the local .env file and we would comment these lines out\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "\n",
    "# We will be using the Tavily web search service (docs.tavily.com/)\n",
    "tavily_search_api_key='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'\n",
    "provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "\n",
    "# List available models and select from allowed models list\n",
    "allowed_models_list=[\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "#allowed_models_list=[\"granite3.2:8b\"]\n",
    "\n",
    "selected_model = None\n",
    "\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "LLAMA_STACK_SERVER='http://localhost:8321'\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMA_STACK_SERVER,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n",
    "    # Check if the model identifier contains any of the allowed substrings\n",
    "    if any(substring in m.identifier for substring in allowed_models_list):\n",
    "        # Only set selected_model if it hasn't been set yet\n",
    "        if selected_model is None:\n",
    "            selected_model = m.identifier\n",
    "           \n",
    "# If no allowed model was found, you might want to handle that case\n",
    "if selected_model is None:\n",
    "    print(\"No allowed model found in the list.\")\n",
    "\n",
    "\n",
    "print(f\"Selected model (from allowed list): {selected_model}\")\n",
    "            # Removed the break here to show all available models, but the selection logic remains picking the first one\n",
    "\n",
    "\n",
    "SELECTED_MODEL = selected_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9de5c-6f9e-43da-aac3-3b43bc62fb50",
   "metadata": {},
   "source": [
    "### Making External Services Available as Tools\n",
    "\n",
    "This is where the magic happens! We need to tell Llama Stack about the external services we just started. By registering them as 'toolgroups', Llama Stack understands their capabilities and can make them available for our agent to discover and use in its problem-solving process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d23404fc-4496-4d65-8f6c-24ead07cb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::mcp-weather\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":\"http://localhost:8005/sse\"},\n",
    "    )\n",
    "\n",
    "client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::mcp-googlemaps\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":\"http://localhost:8006/sse\"},\n",
    "    )\n",
    "client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::mcp-parks-info\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":\"http://localhost:8007/sse\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7811cc-2a4d-4db9-89f2-06a6170ed072",
   "metadata": {},
   "source": [
    "## Building Our Agent's Knowledge Base (RAG)\n",
    "\n",
    "A truly smart agent can also access specific, domain-level knowledge beyond its initial training. We'll achieve this using Retrieval Augmented Generation (RAG). The first step is to create and populate a Vector Database with the information our agent might need – in this case, details about our parks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca62f196-4166-40d3-a819-3b37cf896fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unregistering vector database: Our_Parks_DB\n",
      "Created 5 documents from local files into Our_Parks_DB\n"
     ]
    }
   ],
   "source": [
    "#Unregistering vector database in case we are running this over and over again as part of a learning experience\n",
    "\n",
    "for vector_db_id in client.vector_dbs.list():\n",
    "    print(f\"Unregistering vector database: {vector_db_id.identifier}\")\n",
    "    client.vector_dbs.unregister(vector_db_id=vector_db_id.identifier)\n",
    "\n",
    "# Select a VectorDB provider from availalbe providers\n",
    "providers = client.providers.list()\n",
    "vector_providers = []\n",
    "for provider in client.providers.list():\n",
    "    if provider.api == \"vector_io\":\n",
    "        #print(f\"Found VectorDB provider: {provider.provider_id}\\n\")  # Simple print\n",
    "        vector_providers.append(provider)\n",
    "\n",
    "# In this example, we only have one provider, but on other server we might have many. here, we simply select the first one.\n",
    "selected_vector_provider = vector_providers[0]\n",
    "\n",
    "# register our DB\n",
    "vector_db_id = \"Our_Parks_DB\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=selected_vector_provider.provider_id,\n",
    ")\n",
    "\n",
    "# Injest documents from local directory (this is so you can test inserting changes easily)\n",
    "from llama_stack_client.types import Document\n",
    "\n",
    "files = [\n",
    "    \"Azure_Mongrove_Wilderness.md\",\n",
    "    \"Crimson_Basin.md\",\n",
    "    \"Granite_Spire.md\",\n",
    "    \"Obsidian_Rainforest.md\",\n",
    "    \"Prismatic_Painted_Prairie.md\",\n",
    "]\n",
    "\n",
    "document_dirctory=\"assets/Parks\"\n",
    "\n",
    "# Read documents into the \"documents\" array\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=open(os.path.join(document_dirctory, file), 'r', encoding='utf-8').read(),\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, file in enumerate(files)\n",
    "]\n",
    "\n",
    "# Insert the documents into the vectorDB\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=300,\n",
    ")\n",
    "\n",
    "print(f\"Created {len(documents)} documents from local files into {vector_db_id}\")\n",
    "\n",
    "\n",
    "#print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656d8a7-e759-4bb2-9fe1-afac153960c4",
   "metadata": {},
   "source": [
    "# Summary: Putting it all together - Preparing the Environment\n",
    "\n",
    "In this preparatory section, you've successfully laid the groundwork for building a more advanced agent within the Llama Stack framework. You've taken the essential steps to integrate multiple external services and prepare a knowledge base for your agent to utilize.\n",
    "\n",
    "You have accomplished the following key tasks:\n",
    "\n",
    "* **Deployed Multiple MCP Servers:** You initiated and confirmed the operation of several MCP-enabled containers (`mcp-weather`, `mcp-googlemaps`, `mcp-parks-info`), demonstrating how Llama Stack can connect to a variety of external functionalities.\n",
    "* **Configured the Llama Stack Client:** You set up your Python client, specifying the server address and selecting the appropriate language model that will power your agent.\n",
    "* **Registered External Tools:** You registered the different MCP servers as toolgroups within Llama Stack, making the functions they expose available for discovery and use by agents.\n",
    "* **Established a Knowledge Base:** You created and populated a Vector Database with relevant documents, setting up the critical component for Retrieval Augmented Generation (RAG) to give your agent access to specific information.\n",
    "\n",
    "By completing these steps, your environment is now ready for you to build and experiment with an agent that can combine reasoning, acting with diverse tools (both external services via MCP and internal RAG), and leveraging a custom knowledge base. This is a significant step towards creating more intelligent and capable AI applications with Llama Stack!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08368a3-2cfd-47e7-bbf1-f77d550222e1",
   "metadata": {},
   "source": [
    "### Cleaning Up (Optional)\n",
    "\n",
    "If you need to stop and remove the MCP server containers later, you can use these commands. This is helpful for resetting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33be14de-6a8d-41ac-96be-a9f7adb38eca",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (48070729.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mDON\"T RUN THIS IF YOU DON\"T MEAN TO, IT WILL DELETE YOUR DATABASE\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "DON\"T RUN THIS IF YOU DON\"T MEAN TO, IT WILL DELETE YOUR DATABASE\n",
    "\n",
    "toolgroups_to_unregister = [\n",
    "    \"mcp::mcp-weather\",\n",
    "    \"mcp::mcp-googlemaps\",\n",
    "    \"mcp::mcp-parks-info\",\n",
    "]\n",
    "\n",
    "for toolgroup_id in toolgroups_to_unregister:\n",
    "    try:\n",
    "        print(f\"Attempting to unregister toolgroup: {toolgroup_id}\")\n",
    "        client.toolgroups.unregister(toolgroup_id=toolgroup_id)\n",
    "        print(f\"Successfully unregistered toolgroup: {toolgroup_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to unregister toolgroup {toolgroup_id}. Error: {e}\")\n",
    "\n",
    "print(\"\\nFinished attempting to unregister toolgroups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a931f61-aaa2-462f-b5d4-095b5f736400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686b6b1-02d3-466d-bb99-e614e1fdf62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d198b-3bc6-43d6-80d5-958747ab26e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
