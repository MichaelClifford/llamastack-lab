{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef118175-d75f-4992-8f3c-fc923a3ddffc",
   "metadata": {},
   "source": [
    "# Working with MCP Servers\n",
    "\n",
    "In this section, we will explore the **Model Context Protocol (MCP)** and how Llama Stack enables the seamless integration of external services as powerful tools for your agents. You will learn how to bridge the gap between your LLM applications and domain-specific functionalities or real-time data sources.\n",
    "\n",
    "By the end of this section, you will be able to:\n",
    "\n",
    "* **Understand MCP Servers:** Learn what MCP is and how it allows external services to expose their functionalities to Llama Stack agents.\n",
    "* **Run an MCP-enabled container:** Deploy a pre-built MCP server container that exposes a weather API.\n",
    "* **Register MCP tools:** Register the weather-related functionalities (like `get_alerts` and `get_forecast`) from the MCP server with Llama Stack.\n",
    "* **Utilize MCP tools with a Llama Stack Agent:** Observe how a Llama Stack agent can discover and use these newly registered MCP tools to answer questions requiring external data.\n",
    "\n",
    "This section will demonstrate how Llama Stack extends the capabilities of your AI agents by allowing them to interact with a diverse ecosystem of specialized services, bringing dynamic and real-world data into their decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932625c4-33c3-4e37-b482-5ebfadb97079",
   "metadata": {},
   "source": [
    "**Large Language Models (LLMs)** are incredibly powerful for understanding and generating human-like text. However, by themselves, they are limited to the knowledge they were trained on and cannot interact with the real world, access real-time data, or perform specific actions like looking up weather forecasts or checking inventory.\n",
    "\n",
    "This is where **Model Context Protocols (MCP)** come in. Imagine MCP as a standardized way for *any external service* to describe its functionalities and make them available to an LLM or an AI agent. Think of it like a common language that allows your AI application to say, \"Hey, I need to get the current temperature in London,\" and a separate weather service to understand that request and provide the data, regardless of how that weather service is built internally.\n",
    "\n",
    "Traditionally, connecting an LLM to an external tool involved writing custom wrappers and integration code for each tool and often for each specific LLM framework. This becomes cumbersome and difficult to scale as you add more tools or switch between different AI models or platforms.\n",
    "\n",
    "**MCP addresses this by providing a clear, discoverable interface.** External services, or \"Model Context Protocol Servers,\" expose their capabilities (like a `get_forecast` function or a `check_inventory` function) in a structured, machine-readable format. Llama Stack, for example, can then consume this definition, automatically understand what the tool does, what inputs it needs, and what outputs to expect.\n",
    "\n",
    "This standardized approach offers significant benefits for building scalable AI agents and applications:\n",
    "\n",
    "* **Plug-and-Play Tooling:** Just as you can plug different peripherals into a computer, MCP allows you to \"plug in\" various services as tools for your agents without extensive custom coding for each.\n",
    "* **Enhanced Agent Intelligence:** Agents can transcend their static training data, performing real-time actions, fetching live information, and interacting with enterprise systems, making them far more dynamic and useful.\n",
    "* **Modularity and Maintainability:** Your specialized services (like a weather API or an inventory management system) can evolve independently, and as long as they adhere to the MCP, your AI agents can continue to use them without disruption.\n",
    "* **Scalability:** As your application grows, you can easily add more MCP-enabled services, expanding the agent's capabilities by simply registering the new tools with Llama Stack, rather than rewriting large portions of your AI application.\n",
    "\n",
    "In essence, MCP unlocks the full potential of AI agents by giving them standardized access to the vast world of external data and services, making your AI applications more powerful, adaptable, and easier to manage at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b640ac3-d3a4-4d98-b481-82cc50854d87",
   "metadata": {},
   "source": [
    "### Setting up the environment for our experiment\n",
    "Let's begin by importing necessary utilities for displaying agent steps in a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e775b18-2f36-42e9-89a3-79cb50d74940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import step_printer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ee5ee-068b-4ca4-9b82-a141d76a073d",
   "metadata": {},
   "source": [
    "We will now start our first MCP Server by running the `mcp-weather` container. This server will expose weather-related functionalities that our Llama Stack agent can use as tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e483a08-1c3a-456e-b5fd-3ef92c27f0ed",
   "metadata": {},
   "source": [
    "Run this in a terminal: \n",
    "\n",
    "```bash\n",
    "CONTAINER_NAME=mcp-weather\n",
    "CONTAINER_IMAGE=\"mcp-weather\"\n",
    "CONTAINER_TAG=\"0.1\"\n",
    "REMOTE_REGISTRY=\"quay.dev.demo.redhat.com/rhdp/\"\n",
    "\n",
    "podman run -d \\\n",
    "--name $CONTAINER_NAME \\\n",
    "--network=host \\\n",
    "${REMOTE_REGISTRY}${CONTAINER_IMAGE}:latest \\\n",
    "--port 8005  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b9d0c-844c-44b5-916b-2d133a2808c2",
   "metadata": {},
   "source": [
    "Once the MCP server is running, we can quickly verify its availability by attempting to connect to its `/sse` (Server-Sent Events) endpoint. A successful response indicates the server is live.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464448de-8a1f-4fcd-a536-d7f78971b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl --max-time 1 http://localhost:8005/sse 2>/dev/null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae04622-7ff4-45b0-a088-9d0a0bace44b",
   "metadata": {},
   "source": [
    "### Setting up Llama Stack Agent (Stuff we already know)\n",
    "Now, let's set up our Llama Stack client. This involves importing necessary libraries, configuring the Llama Stack server URL, and selecting the language model our agent will use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2ab1d-cae2-4547-8c54-4d33023c7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# These libraries are just here to print the results from the agent in a more human-readable way \n",
    "from termcolor import cprint\n",
    "import uuid\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "stream=False ## Defaulting to False, you can change this throughout the section to \"True\" if you wanted to see the output in another format (Using EventLogger)\n",
    "\n",
    "# for our lab, we will just define our variables manualy here, in a regular application, this would be ready directly from the local .env file and we would comment these lines out\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "\n",
    "# We will be using the Tavily web search service (docs.tavily.com/)\n",
    "tavily_search_api_key='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'\n",
    "provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "\n",
    "# List available models and select from allowed models list\n",
    "allowed_models_list=[\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "selected_model = None\n",
    "\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "LLAMA_STACK_SERVER='http://localhost:8321'\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMA_STACK_SERVER,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n",
    "    # Check if the model identifier contains any of the allowed substrings\n",
    "    if any(substring in m.identifier for substring in allowed_models_list):\n",
    "        # Only set selected_model if it hasn't been set yet\n",
    "        if selected_model is None:\n",
    "            selected_model = m.identifier\n",
    "           \n",
    "# If no allowed model was found, you might want to handle that case\n",
    "if selected_model is None:\n",
    "    print(\"No allowed model found in the list.\")\n",
    "\n",
    "\n",
    "print(f\"Selected model (from allowed list): {selected_model}\")\n",
    "            # Removed the break here to show all available models, but the selection logic remains picking the first one\n",
    "\n",
    "\n",
    "SELECTED_MODEL = selected_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e0afe-daf0-4ec8-9609-d2f5a4adbee9",
   "metadata": {},
   "source": [
    "### Registering the new MCP Server as a tool\n",
    "Before registering our new MCP tools, let's inspect the tools currently available to Llama Stack. These are typically built-in tools or tools from previously registered providers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c312ab-b341-4110-a539-24b3a28fd00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "\n",
    "for tools in registered_tools:\n",
    "    print(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c4345-adb4-4177-9c13-5debefde354b",
   "metadata": {},
   "source": [
    "This is a crucial step: we are now registering our MCP Weather server as a 'toolgroup' with Llama Stack. This makes the functionalities exposed by the `mcp-weather` container (`get_alerts`, `get_forecast`) available for our agents to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04b7e0-b8e5-41ad-a5f9-c71d027e78c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::mcp-weather\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":\"http://localhost:8005/sse\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88320187-a4b4-432b-bc23-fa77fe0509ab",
   "metadata": {},
   "source": [
    "After registration, let's list the available tools again. You should now see the weather-related tools (`get_alerts`, `get_forecast`) exposed by our MCP server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da12894-0b17-4b4f-8622-513246722783",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "\n",
    "for tools in registered_tools:\n",
    "    print(\"\\n\")\n",
    "    print(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffc708-077f-446f-97c4-24dfee6ce623",
   "metadata": {},
   "source": [
    "Now, we'll define our Llama Stack agent. We'll instruct it to be a helpful agent and specifically grant it access to the `mcp::mcp-weather` toolgroup we just registered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e96f7e-5578-49ca-9ddd-467cbb66e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    client, \n",
    "    model=SELECTED_MODEL,\n",
    "    instructions=\"\"\"You are a helpful agent with access to tools, use the weather tool to answer questions\n",
    "            \"\"\" ,\n",
    "    tools=[\"mcp::mcp-weather\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bebf4c9-eef2-4c4e-8dc3-40dead09d344",
   "metadata": {},
   "source": [
    "Finally, let's test our agent! We'll provide it with a few prompts that require it to use the newly available weather tools to retrieve information.\n",
    "\n",
    "Sources and related content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5bf65-d210-4547-87d8-5d850ec5dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream=False\n",
    "user_prompts = [\n",
    "       \"what is the weather in boulder colorado\",\n",
    "       \"are there any weather alerts for Boston at the moment?\",\n",
    "]\n",
    "\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    # Generate a new Unique Identifier for each session \n",
    "    new_uuid = uuid.uuid4()\n",
    "    session_id = agent.create_session(f\"web-session-{new_uuid}\")\n",
    "\n",
    "    cprint(f\"\\n{'='*100}\\nProcessing user query: {prompt}\\n{'='*100}\", \"blue\")\n",
    "\n",
    "    \n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77a8e0-6d55-416d-9333-08efef6ec47d",
   "metadata": {},
   "source": [
    "### Analyzing the Agent's Execution\n",
    "\n",
    "Let's break down the output you just saw to understand how our Llama Stack agent, now empowered by the MCP Weather tools, processed your queries:\n",
    "\n",
    "For the query \"**what is the weather in boulder colorado**\":\n",
    "\n",
    "* **Step 1 (InferenceStep):** The agent receives the query. Its intelligence determines that to answer this question, it needs weather *forecast* data. It identifies the `get_forecast` tool (exposed via MCP) as relevant and intelligently extracts the latitude and longitude for \"Boulder, Colorado\" to form the tool call arguments (`{'latitude': '40.0116', 'longitude': '-105.2729'}`).\n",
    "* **Step 2 (ToolExecutionStep):** The `get_forecast` tool is executed by Llama Stack, which communicates with our running `mcp-weather` server. The **Observation** is the detailed weather forecast data returned by the MCP server, including \"Today: Temperature: 85°F...Tonight: Temperature: 58°F...\".\n",
    "* **Step 3 (InferenceStep):** The agent processes the raw forecast data (the Observation from Step 2). Its final thought is to synthesize this information into a concise, human-readable **Model Response** that directly answers the user's question about the weather in Boulder, including the forecast for the coming days.\n",
    "\n",
    "Similarly, for the query \"**are there any weather alerts for Boston at the moment?**\":\n",
    "\n",
    "* **Step 1 (InferenceStep):** The agent understands the request is for weather *alerts*. It recognizes the `get_alerts` tool (also exposed via MCP) as the appropriate tool. It then determines the two-letter state code for \"Boston\" (which is 'MA' for Massachusetts) and constructs the tool call (`{'state': 'MA'}`).\n",
    "* **Step 2 (ToolExecutionStep):** The `get_alerts` tool is executed, again via the `mcp-weather` server. The **Observation** in this case indicates `{'type': 'text', 'text': 'No active alerts for this state.'}`.\n",
    "* **Step 3 (InferenceStep):** Based on the tool's output, the agent formulates a direct and clear **Model Response**, stating that there are no current weather alerts for Massachusetts.\n",
    "\n",
    "This flow clearly illustrates how the Llama Stack agent dynamically selected the correct MCP tool based on the user's intent, executed it, and then synthesized the results from the external service into a natural language response. This dynamic interaction with external services is a core capability enabled by the Model Context Protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130dc9c2-2ed6-4087-828e-53ec306489ae",
   "metadata": {},
   "source": [
    "## Lab Summary: Working with MCP Servers\n",
    "\n",
    "In this lab, you gained hands-on experience with the **Model Context Protocol (MCP)** and its integration within the Llama Stack framework. You learned how MCP serves as a vital bridge, allowing your Llama Stack agents to leverage external, specialized services as integral tools in their problem-solving workflows.\n",
    "\n",
    "Through the exercises, you learned to:\n",
    "\n",
    "* **Deploy an MCP Server:** You successfully ran a `mcp-weather` container, acting as an external service providing weather-related functionalities.\n",
    "* **Connect MCP to Llama Stack:** You saw how Llama Stack's architecture facilitates the registration of MCP endpoints, making their exposed functions discoverable by agents.\n",
    "* **Integrate External Tools:** You registered specific weather tools (`get_alerts` and `get_forecast`) from your MCP server with Llama Stack.\n",
    "* **Empower Agents with External Data:** You observed a Llama Stack agent dynamically using these newly available MCP tools to retrieve real-time weather forecasts and alerts, directly responding to user queries.\n",
    "\n",
    "This experience highlights how Llama Stack simplifies the process of extending agent capabilities by integrating domain-specific tools via MCP. It showcases the power of a unified API that abstracts away the complexities of diverse external services, enabling developers to build sophisticated, context-aware AI applications with remarkable ease and flexibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
