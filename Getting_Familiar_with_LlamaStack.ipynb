{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6089d285-a329-4f61-9edc-3dff39015877",
   "metadata": {},
   "source": [
    "Let's start by installing the Python Libraries we neeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7268ad4-bda8-427c-9320-3c836e861887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-stack-client in /opt/conda/lib/python3.11/site-packages (0.1.9)\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (4.0.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (8.1.8)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (0.28.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (2.2.3)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (3.0.39)\n",
      "Requirement already satisfied: pyaml in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (25.1.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (2.11.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (14.0.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (1.3.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (3.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.11/site-packages (from llama-stack-client) (4.13.0)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->llama-stack-client) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->llama-stack-client) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->llama-stack-client) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama-stack-client) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->llama-stack-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->llama-stack-client) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->llama-stack-client) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-stack-client) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-stack-client) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-stack-client) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-stack-client) (2025.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit->llama-stack-client) (0.2.8)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from pyaml->llama-stack-client) (6.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->llama-stack-client) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->llama-stack-client) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack-client) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client) (1.16.0)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-stack-client dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62bc6b-c7a9-40a5-93ff-a0b8dfd75851",
   "metadata": {},
   "source": [
    "When running this code in a regular Python application, we would usually like to read environment variables from an `.env` file, for our needs in this lab, we will hard code these in this cell, to make things more clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3a729f5-6954-4ffc-844c-5b758204a235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for our lab, we will just define our variables manualy here:\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "os.environ['LLAMA_STACK_MODEL'] = 'meta-llama/Llama-3.2-3B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2008b-85be-4be6-9e05-82cc25aac275",
   "metadata": {},
   "source": [
    "As a first step, let's define our client, provide it our Llama-Stack Server location and select the model we would like to work with, later, we will see that pointing this to a different location (Llama-Stack Serve) is all we would need to do to move to a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "193a3474-d8d9-4ea9-a205-61e5a4b46dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Available models: ---\n",
      "all-MiniLM-L6-v2 - ollama - all-minilm:latest\n",
      "meta-llama/Llama-3.2-3B-Instruct - ollama - llama3.2:3b-instruct-fp16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "LLAMA_STACK_MODEL=os.getenv(\"LLAMA_STACK_MODEL\")\n",
    "\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_SERVER)\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfc89d-a581-4a0d-91dc-849e9849b822",
   "metadata": {},
   "source": [
    "Now that our client is set up, let's go through some very simple code snippets, to get you familiar with the syntex. If you used other AI Frameworks, this will soon feel very familiar, as Llamastack follows similar principals and terminology, while allowing a standard to help you quickly shift different components in and out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80a26696-c9dd-4efd-ae01-a1750e8609a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top speed of a leopard can vary depending on several factors, such as the individual animal's age, sex, and motivation (e.g., chasing prey or escaping from predators). However, according to various sources, including the National Geographic and the World Wildlife Fund, the average running speed of a leopard is around 40-50 km/h (25-31 mph).\n",
      "\n",
      "However, some leopards have been clocked at speeds of up to 60 km/h (37 mph) over short distances, such as when chasing prey or escaping from danger. In fact, one study found that a male leopard can reach speeds of up to 70 km/h (43.5 mph) for brief periods.\n",
      "\n",
      "It's worth noting that leopards are agile and powerful climbers, and they often use their speed and agility to navigate through dense forests and rocky terrain with ease.\n"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=LLAMA_STACK_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the top speed of a leopard?\",\n",
    "        },\n",
    "    ],\n",
    "    # temperature=0.0, \n",
    ")\n",
    "print(response.completion_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91507f-ed66-49a1-8ed5-281b98c1ea7e",
   "metadata": {},
   "source": [
    "Often, we want the LLM to provide us a specific answer, not in a conversational manner, using structured data can be helpful, later in the lab, you will see how we want certain agents to give us specific facts and not a short story about the facts.\n",
    "\n",
    "Try different animals, to see how the structured data can be helpful for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c7b05a2-8e58-4e83-af0e-9de864580a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Speed:  80\n",
      "Animal:  leopard\n",
      "metric_type:  km/h\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class AnimalSpeed(BaseModel):\n",
    "    speed: int\n",
    "    animal: str\n",
    "    metric_type: str\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=LLAMA_STACK_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the top speed of a leopard?\",            \n",
    "        },\n",
    "    ],\n",
    "    stream=False,    \n",
    "    response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": AnimalSpeed.model_json_schema(),\n",
    "        }\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    response_data = json.loads(response.completion_message.content)\n",
    "    animal = AnimalSpeed(**response_data)    \n",
    "    print(\"-------\")\n",
    "    print(\"Speed: \", animal.speed)\n",
    "    print(\"Animal: \", animal.animal)\n",
    "    print(\"metric_type: \", animal.metric_type)\n",
    "    print(\"-------\")\n",
    "except (json.JSONDecodeError, ValueError) as e:\n",
    "    print(f\"Invalid format: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d677ceb-9135-4d54-91c7-eaeda5825ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
