{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a656d8a7-e759-4bb2-9fe1-afac153960c4",
   "metadata": {},
   "source": [
    "# Putting it all together - Agent Execution\n",
    "\n",
    "Building on our previous steps where we set up the Llama Stack environment, registered MCP tools, and established our Vector Database for RAG, this section brings all these components together.\n",
    "\n",
    "We will now focus on **Agent Execution**, demonstrating how a sophisticated AI agent, powered by the ReAct framework, can leverage these resources to process complex queries. This is where you will see the combined power of the Llama Stack ecosystem in action.\n",
    "\n",
    "In this section, you will:\n",
    "\n",
    "* **Configure a ReAct agent** designed to work with multiple toolgroups and a tailored prompt.\n",
    "* **Observe the agent's decision-making process** as it selects and utilizes different tools based on the user's input.\n",
    "* **Examine the agent's \"Chain of Thought\"** to understand how it approaches and breaks down problems.\n",
    "* **See how RAG is integrated** to provide the agent with access to domain-specific knowledge for informed responses.\n",
    "* **Understand the synergy** of combining Llama Stack's core components to build intelligent, multi-capability applications.\n",
    "\n",
    "Let's connect the pieces and run our advanced Llama Stack agent!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75373f1c-e200-4bd5-9a6f-48b40833e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U llama-stack-client==0.2.5 dotenv > /dev/null 2>&1 && echo \"pip Python Prerequisites installed succesfuly\"\n",
    "\n",
    "import os\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# These libraries are just here to print the results from the agent in a more human-readable way \n",
    "from src.utils import step_printer\n",
    "#from src.client_tools import get_location\n",
    "from termcolor import cprint\n",
    "import uuid\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "stream=False ## Defaulting to False, you can change this throughout the section to \"True\" if you wanted to see the output in another format (Using EventLogger)\n",
    "\n",
    "# for our lab, we will just define our variables manualy here, in a regular application, this would be ready directly from the local .env file and we would comment these lines out\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "from llama_stack_client import LlamaStackClient\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMA_STACK_SERVER,\n",
    ")\n",
    "\n",
    "# List available models and select from allowed models list\n",
    "allowed_models_list=[\"granite3.2:8b\"]\n",
    "selected_model = None\n",
    "\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n",
    "    # Check if the model identifier contains any of the allowed substrings\n",
    "    if any(substring in m.identifier for substring in allowed_models_list):\n",
    "        # Only set selected_model if it hasn't been set yet\n",
    "        if selected_model is None:\n",
    "            selected_model = m.identifier\n",
    "           \n",
    "# If no allowed model was found, you might want to handle that case\n",
    "if selected_model is None:\n",
    "    print(\"No allowed model found in the list.\")\n",
    "print(f\"Selected model (from allowed list): {selected_model}\")\n",
    "model = selected_model\n",
    "\n",
    "vector_db_id = \"Our_Parks_DB\"\n",
    "query_config = {\n",
    "    \"query_generator_config\": {\n",
    "        \"type\": \"default\",\n",
    "        \"separator\": \" \"\n",
    "    },\n",
    "    \"max_tokens_in_context\": 300,\n",
    "    \"max_chunks\": 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51fa587-2910-4d52-b11a-f0b129dc7207",
   "metadata": {},
   "source": [
    "### Customizing the Agent's Instructions (The Prompt)\n",
    "\n",
    "The prompt serves as the core instructions for our AI agent, guiding its behavior and strategy. While Llama Stack provides default prompts, we have the flexibility to define our own to precisely control how the agent operates and interacts with its tools.\n",
    "\n",
    "In this step, we will **define a custom prompt string**. This string includes placeholders (`<<tool_names>>`, `<<tool_descriptions>>`) where we will dynamically insert the details of the tools available to the agent. This explicit approach ensures the agent is fully aware of its capabilities and the specific rules we define for tool usage.\n",
    "\n",
    "This exercise highlights how you can tailor the agent's foundational instructions to meet specific application requirements.\n",
    "\n",
    "> **Note:**\n",
    "> To see how the default ReAct agent prompt is structured, you can explore the source code [here](https://github.com/meta-llama/llama-stack-client-python/blob/main/src/llama_stack_client/lib/agents/react/prompts.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba28750-51f5-44c0-af99-3c44f38395fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_react_prompt = \"\"\"\n",
    "\n",
    "You are an expert assistant who can solve any task using tool calls. You will be given a task to solve as best you can.\n",
    "\n",
    "To do so, you have been given access to the following tools: <<tool_names>>\n",
    "\n",
    "üö® TOOL USAGE RULES ‚Äî FOLLOW STRICTLY üö®\n",
    "\n",
    "1. ‚ùå Do NOT guess or invent tool names.\n",
    "2. ‚úÖ You may only call tools that are explicitly listed in <<tool_names>>.\n",
    "3. üõë Never use tools like `Maps`, `search`, `maps_geocode`, or `knowledge_search` unless they are explicitly in <<tool_names>>.\n",
    "4. If you cannot solve a task using the tools available, explain that limitation in your final answer instead of calling an invalid tool.\n",
    "5. üîé ALWAYS ALWAYS ALWAYS USE `get_park_location` before `get_alerts`\n",
    "6. ‚ùì If a tool needs location data (like `get_alerts`, `maps_search_places`), you must first call `get_park_location` and extract the relevant state, city, or coordinates from its result before proceeding. You must not hardcode, assume, or guess this data.\n",
    "7. üß† Internally keep track of whether you have already retrieved park location. Do not call `get_alerts` until this location has been confirmed via `get_park_location`.\n",
    "\n",
    "üõ¶ TOOL PARAMETER FORMAT (MANDATORY):\n",
    "Each tool call must use the following format for `tool_params`:\n",
    "```json\n",
    "\"tool_params\": [\n",
    "  {\"name\": \"parameter_name\", \"value\": \"actual value\"}\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "üßê RESPONSE FORMAT (ALWAYS JSON):\n",
    "\n",
    "{\n",
    "    \"thought\": $THOUGHT_PROCESS,\n",
    "    \"action\": {\n",
    "        \"tool_name\": $TOOL_NAME,\n",
    "        \"tool_params\": $TOOL_PARAMS\n",
    "    },\n",
    "    \"answer\": $ANSWER\n",
    "}\n",
    "\n",
    "Only one tool may be called at a time. Use multiple steps when needed.\n",
    "\n",
    "Use `\"action\": null` and set `answer` when you‚Äôre ready to respond to the user.\n",
    "\n",
    "---\n",
    "\n",
    "üóìÔ∏è EXAMPLE:\n",
    "\n",
    "Task: ‚ÄúAre there any supermarkets near Crimson Basin?‚Äù\n",
    "\n",
    "Step 1:\n",
    "{\n",
    "    \"thought\": \"I need to get the location of Crimson Basin park using get_park_location.\",\n",
    "    \"action\": {\n",
    "        \"tool_name\": \"get_park_location\",\n",
    "        \"tool_params\": [\n",
    "            {\"name\": \"park_name\", \"value\": \"Crimson Basin\"}\n",
    "        ]\n",
    "    },\n",
    "    \"answer\": null\n",
    "}\n",
    "\n",
    "Observation: {\"result\": \"Crimson Basin is located in Nevada, USA.\"}\n",
    "\n",
    "Step 2:\n",
    "{\n",
    "    \"thought\": \"Now I will search for supermarkets near that location using maps_search_places.\",\n",
    "    \"action\": {\n",
    "        \"tool_name\": \"maps_search_places\",\n",
    "        \"tool_params\": [\n",
    "            {\"name\": \"query\", \"value\": \"supermarkets near Nevada, USA\"}\n",
    "        ]\n",
    "    },\n",
    "    \"answer\": null\n",
    "}\n",
    "\n",
    "Observation: { ... list of stores ... }\n",
    "\n",
    "Final step:\n",
    "{\n",
    "    \"thought\": \"I now have the info the user asked for.\",\n",
    "    \"action\": null,\n",
    "    \"answer\": \"There are several supermarkets near Crimson Basin, including WinCo Foods, Albertsons, and Smith‚Äôs.\"\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "‚ùó IF A TOOL IS MISSING:\n",
    "\n",
    "If no tool is available for the task:\n",
    "{\n",
    "    \"thought\": \"I need to get nearby supermarkets, but I do not have access to a maps tool.\",\n",
    "    \"action\": null,\n",
    "    \"answer\": \"I‚Äôm unable to find supermarkets near Crimson Basin because I don‚Äôt have access to a maps tool.\"\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "You only have access to the following tools: <<tool_descriptions>>\n",
    "\n",
    "SUMMARY:\n",
    "- Do not guess tools. Use only the ones listed.\n",
    "- Follow the exact `tool_params` format.\n",
    "- Use get_park_location to resolve any location before calling tools that require geographic input.\n",
    "- Do not hardcode, guess, or assume state names or coordinates.\n",
    "- Always use get_park_location before get_alerts.\n",
    "- Keep internal memory of what locations have already been retrieved.\n",
    "- Return final answers with `\"action\": null`.\n",
    "\n",
    "Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627effb-f0cb-46ef-96f8-12e5d2b56795",
   "metadata": {},
   "source": [
    "### Identifying Available Tools for the Agent\n",
    "\n",
    "Now that we have our custom prompt defined, we need to determine which tools the agent will have at its disposal. Llama Stack makes it straightforward to manage tools registered via MCPs or other sources.\n",
    "\n",
    "In this step, we will **retrieve the list of available tools** from the Llama Stack server. When MCP servers are registered, Llama Stack interacts with them to automatically discover and catalog the tools they expose, including their detailed descriptions and how to use them (parameters, etc.).\n",
    "\n",
    "We will then filter this complete list to select only the specific toolgroups required for our agent in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd22f73-e6a0-4e76-bd6e-1c68fa63893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_tools = client.tools.list()\n",
    "\n",
    "\n",
    "allowed_toolgroups = [\n",
    "    'mcp::mcp-weather',\n",
    "    'mcp::mcp-googlemaps',\n",
    "    'mcp::mcp-parks-info',\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the dictionaries for each tool\n",
    "allowed_tools_array = []\n",
    "\n",
    "for tool in registered_tools:\n",
    "    if tool.toolgroup_id in allowed_toolgroups:\n",
    "        #print(tool)\n",
    "        allowed_tools_array.append(tool)\n",
    "\n",
    "# If you want to see what this looks like, uncomment these lines\n",
    "#print(\"List of allowed tools (each as a dictionary):\")\n",
    "#print(allowed_tools_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b9975-95d8-4f60-baf2-a8a71c6a6e91",
   "metadata": {},
   "source": [
    "### Dynamically Injecting Tool Information\n",
    "\n",
    "Normally, Llama Stack handles the integration of tool information into the agent's context automatically. However, since we are using a completely custom prompt in this lab, we need to manually perform this step.\n",
    "\n",
    "This process demonstrates the flexibility Llama Stack offers. By manually injecting the tool names and descriptions (which we retrieved in the previous step) into the `<<tool_names>>` and `<<tool_descriptions>>` placeholders in our custom prompt string, we are explicitly providing the agent with the definition of its capabilities.\n",
    "\n",
    "This ensures that the agent, guided by our custom instructions and now equipped with knowledge of its available tools, is ready to process user requests intelligently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe2fa2-9aba-4a53-8db3-1c0da620d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def insert_tools_to_prompt(my_instructions: str, allowed_tools_array: list) -> str:\n",
    "    \"\"\"\n",
    "    Formats the source template string by inserting tool names and descriptions\n",
    "    from a list of Tool objects.\n",
    "\n",
    "    Args:\n",
    "        my_instructions: A multi-string variable containing the source template.\n",
    "                         Expected to have <<tool_names>> and <<tool_descriptions>> placeholders.\n",
    "        allowed_tools_array: A list of Tool objects obtained from the client library.\n",
    "\n",
    "    Returns:\n",
    "        A multi-string variable with placeholders replaced by formatted tool information.\n",
    "    \"\"\"\n",
    "    tool_names = []\n",
    "    tool_descriptions = []\n",
    "\n",
    "    for tool in allowed_tools_array:\n",
    "        tool_names.append(tool.identifier)\n",
    "        formatted_parameters = []\n",
    "        for param in tool.parameters:\n",
    "            # Escape single quotes in the parameter description for the string literal\n",
    "            param_description_escaped = param.description.replace(\"'\", \"\\\\'\")\n",
    "            formatted_parameters.append(\n",
    "                f\"Parameter(description='{param_description_escaped}', name='{param.name}', parameter_type='{param.parameter_type}', required={param.required}, default={param.default})\"\n",
    "            )\n",
    "        cleaned_description = tool.description.replace('\\\\n', '\\n').replace(\"    \", \"\").replace(\"'\", \"\\\\'\")\n",
    "\n",
    "        tool_descriptions.append(\n",
    "            f\"- {tool.identifier}: {{'name': '{tool.identifier}', 'description': '{cleaned_description}', 'parameters': [{', '.join(formatted_parameters)}]}}\"\n",
    "        )\n",
    "\n",
    "    tool_names_string = \", \".join(tool_names)\n",
    "\n",
    "    tool_descriptions_string = \"\\n\".join(tool_descriptions)\n",
    "\n",
    "    output_template = my_instructions.replace(\"<<tool_names>>\", tool_names_string).replace(\"<<tool_descriptions>>\", tool_descriptions_string)\n",
    "\n",
    "    return output_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd3503-383b-4cd1-b196-e21925854191",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_react_prompt_with_tools=insert_tools_to_prompt(custom_react_prompt,allowed_tools_array)\n",
    "print(custom_react_prompt_with_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa09b0f-d438-49d7-8be9-4e4f59937d63",
   "metadata": {},
   "source": [
    "## Unleashing the Custom ReAct Agent\n",
    "\n",
    "Alright, the stage is set! We have our Llama Stack server, our specialized MCP tools are registered, our knowledge base is ready, and we've crafted a custom prompt loaded with tool definitions.\n",
    "\n",
    "Now, it's time to bring our **ReAct agent** to life and see it in action! In this section, we'll instantiate the `ReActAgent` class with our configurations and run a few example queries. Pay close attention to the output ‚Äì you'll be able to follow the agent's reasoning process and how it decides to use the tools we provided.\n",
    "\n",
    "*(That's where the \"ReAct\" name comes from ‚Äì it's a framework focused on the agent's **Reasoning** and **Acting** by calling tools!)*\n",
    "\n",
    "> **Important Considerations:**\n",
    "> \n",
    "> Keep in mind that the agent's responses can vary based on the specific question, the underlying language model, the prompt design, and even settings like temperature. This exercise is designed to illustrate the *potential* of building intelligent agents with Llama Stack. Creating a production-ready, perfectly conversational agent often involves further iteration and fine-tuning of these components. Embrace the journey and observe the process!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a3f02-9c0c-4926-83c6-e67fad1d4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
    "\n",
    "stream=False\n",
    "\n",
    "\n",
    "agent = ReActAgent(\n",
    "            client=client,\n",
    "            model=model,\n",
    "            instructions=custom_react_prompt_with_tools,\n",
    "            tools=[\"mcp::mcp-parks-info\",\"mcp::mcp-weather\",\"mcp::mcp-googlemaps\"],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": ReActOutput.model_json_schema(),\n",
    "            },\n",
    "            #sampling_params=sampling_params,\n",
    "        )\n",
    "user_prompts = [\n",
    "    #\"are there any hotels on the drive from Las Vegas, Nevada to Crimson Basin park\",\n",
    "     \"Are there any supermarkets near Crimson Basin park?\", # Works\n",
    "    # \"Should I be worried about severe weather conditions around Azure Mongrove Wilderness park today?\",\n",
    "    # \"Can you find a pharmacy close to Prismatic Painted Prairie park?\",\n",
    "    # \"what are the coordinates of Prismatic Painted Prairie park?\",\n",
    "]\n",
    "\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    new_uuid = uuid.uuid4()\n",
    "    session_id = agent.create_session(f\"React-session-{new_uuid}\")\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75282ae1-b02d-4cee-8ca6-32f84e1042bf",
   "metadata": {},
   "source": [
    "# Putting it all together - Agent Execution\n",
    "\n",
    "With our environment successfully prepared in the previous section, it's time for the main event! In this part of the lab, we will take all the pieces we've assembled ‚Äì our Llama Stack server, the registered MCP tools providing external capabilities, our populated Vector Database for RAG, and our understanding of ReAct agents ‚Äì and put them into action.\n",
    "\n",
    "This section is where you'll see the true power of Llama Stack agents come to life as we build and execute a sophisticated agent capable of reasoning, interacting with multiple external services, and accessing domain-specific knowledge to answer complex queries.\n",
    "\n",
    "By working through this section, you will:\n",
    "\n",
    "* See how to configure a ReAct agent with multiple toolgroups and a custom prompt.\n",
    "* Understand how an agent selects and utilizes different tools based on the user's request.\n",
    "* Observe the agent's \"Chain of Thought\" as it breaks down a complex problem.\n",
    "* Experience how RAG is integrated into the agent's workflow to provide informed responses.\n",
    "* Appreciate the flexibility and power of combining various Llama Stack components to build intelligent applications.\n",
    "\n",
    "Let's connect everything and unleash the potential of our multi-talented Llama Stack agent!\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
