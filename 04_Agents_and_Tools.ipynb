{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f75a76c-cb61-4c32-a334-a5bcd41f4e7d",
   "metadata": {},
   "source": [
    "# Agents and Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089d285-a329-4f61-9edc-3dff39015877",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Install Python Prerequisist\n",
    "\n",
    "As always, let's start by installing the Python Libraries we neeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7268ad4-bda8-427c-9320-3c836e861887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U llama-stack-client==0.2.5 dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62bc6b-c7a9-40a5-93ff-a0b8dfd75851",
   "metadata": {},
   "source": [
    "### Define the LLamastack server and Model\n",
    "\n",
    "Let's point our variables to our Llamastack server and chose our desired model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a729f5-6954-4ffc-844c-5b758204a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for our lab, we will just define our variables manualy here:\n",
    "## if you are running from jupyter-nodes server or doing a lab with RHDP:\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "## If you are running from a jupyter-notes container:\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://host.containers.internal:8321'\n",
    "os.environ['LLAMA_STACK_MODEL'] = 'meta-llama/Llama-3.2-3B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2008b-85be-4be6-9e05-82cc25aac275",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">When running this code in a regular Python application, we would usually like to read environment variables from an `.env` file, for our needs in this lab, we will hard code these in this cell, to make things more clear\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5dad55-f81f-482a-8080-85115078f8b8",
   "metadata": {},
   "source": [
    "### Initialize the *Client* \n",
    "As a first step, let's define our client, provide it our Llama-Stack Server location and select the model we would like to work with, later, we will see that pointing this to a different location (Llama-Stack Serve) is all we would need to do to move to a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_search_api_key='tvly-BROKENdBROKENf89fUfZMUSIe'\n",
    "\n",
    "provider_data = {\"tavily_search_api_key\": tavily_search_api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a3474-d8d9-4ea9-a205-61e5a4b46dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "LLAMA_STACK_MODEL=os.getenv(\"LLAMA_STACK_MODEL\")\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMA_STACK_SERVER,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97436930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    client, \n",
    "    model='meta-llama/Llama-3.2-3B-Instruct',\n",
    "    #model='meta-llama/Llama-3.1-8B-Instruct',\n",
    "    instructions=\"\"\"You are a helpful websearch assistant. When you are asked to search the latest you must use a tool. \n",
    "            Whenever a tool is called, be sure return the response in a friendly and helpful tone.\n",
    "            \"\"\" ,\n",
    "    tools=[\"builtin::websearch\"],\n",
    "    #sampling_params=sampling_params\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707dd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "stream=True\n",
    "user_prompts = [\n",
    "       \"search the web for the latest in OpenShift?\",\n",
    "       \"search for the current weather in boston\",\n",
    "]\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    session_id = agent.create_session(\"web-session\")\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a3e9d",
   "metadata": {},
   "source": [
    "Wow look at that, notice that the agent is kinda silly.. if you try a nother word, it might fail, thats more a symptom of a small LLM than the agent... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb674e6",
   "metadata": {},
   "source": [
    "Now lets try REact agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The next cell is meant to halucinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa9d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"Search for any weather-related risks in my area that could disrupt network connectivity or system availability?\",\n",
    "]\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    session_id = agent.create_session(\"web-session1\")\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ee3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geocoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd224b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.client_tool import client_tool\n",
    "\n",
    "@client_tool\n",
    "def get_location(query: str = \"location\"):\n",
    "    \"\"\"\n",
    "    Provides the user's location upon request.\n",
    "\n",
    "    This function uses the geocoder library to determine the user's location\n",
    "    based on their IP address.  It returns the city, state, and country.\n",
    "\n",
    "    :param query:  The query from the user.  Defaults to \"location\".\n",
    "    :type query: str\n",
    "    :return:  Information about the user's current location.\n",
    "    :rtype: str\n",
    "\n",
    "    Example:\n",
    "        >>> get_location(\"where am i\")\n",
    "        \"Your current location is: Some City, Some State, Some Country\"\n",
    "    \"\"\"\n",
    "    import geocoder\n",
    "    try:\n",
    "        g = geocoder.ip('me')\n",
    "        if g.ok:\n",
    "            return f\"Your current location is: {g.city}, {g.state}, {g.country}\" # can be modified to return latitude and longitude if needed\n",
    "        else:\n",
    "            return \"Unable to determine your location\"\n",
    "    except Exception as e:\n",
    "        return f\"Error getting location: {str(e)}\"\n",
    "#test_geo = get_location(\"where am i\")\n",
    "#print(test_geo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = Agent(\n",
    "    client, \n",
    "    model='meta-llama/Llama-3.2-3B-Instruct',\n",
    "    #model='meta-llama/Llama-3.1-8B-Instruct',\n",
    "    instructions=\"\"\"You are a helpful assistant. \n",
    "    When a user asks about their location, you MUST use the get_location tool. When searching for nearby places, you MUST use the websearch tool.\n",
    "    \"\"\" ,\n",
    "    tools=[get_location, \"builtin::websearch\"],\n",
    "    #sampling_params=sampling_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e3a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream=True\n",
    "user_prompts = [\n",
    "    \"Where am I?\",\n",
    "    \"Are there any weather-related risks in my area that could disrupt network connectivity or system availability?\",\n",
    "    \"Search for the current weather near me\"\n",
    "]\n",
    "session_id = agent.create_session(\"prompt-chaining-session\")  # for prompt chaining, queries must share the same session_id.\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab70a83",
   "metadata": {},
   "source": [
    "The above doesn't work properly yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e3788",
   "metadata": {},
   "source": [
    "## ReActAGent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f998fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed19246",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent(\n",
    "            client=client,\n",
    "            model='meta-llama/Llama-3.2-3B-Instruct',\n",
    "            #model='meta-llama/Llama-3.1-8B-Instruct',\n",
    "            tools=[get_location, \"builtin::websearch\"],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": ReActOutput.model_json_schema(),\n",
    "            },\n",
    "            #sampling_params=sampling_params,\n",
    "        )\n",
    "user_prompts = [\n",
    "    \"search for any weather-related risks near my location\"\n",
    "]\n",
    "session_id = agent.create_session(\"web-session\")\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678dd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df377264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a0707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187dd20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3de0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
