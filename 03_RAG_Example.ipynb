{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6089d285-a329-4f61-9edc-3dff39015877",
   "metadata": {},
   "source": [
    "Let's start by installing the Python Libraries we neeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efcb5a7",
   "metadata": {},
   "source": [
    "https://github.com/meta-llama/llama-stack/blob/main/docs/notebooks/Llama_Stack_RAG_Lifecycle.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7268ad4-bda8-427c-9320-3c836e861887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -U llama-stack-client dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62bc6b-c7a9-40a5-93ff-a0b8dfd75851",
   "metadata": {},
   "source": [
    "When running this code in a regular Python application, we would usually like to read environment variables from an `.env` file, for our needs in this lab, we will hard code these in this cell, to make things more clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a729f5-6954-4ffc-844c-5b758204a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for our lab, we will just define our variables manualy here:\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "os.environ['LLAMA_STACK_MODEL'] = 'meta-llama/Llama-3.2-3B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2008b-85be-4be6-9e05-82cc25aac275",
   "metadata": {},
   "source": [
    "As a first step, let's define our client, provide it our Llama-Stack Server location and select the model we would like to work with, later, we will see that pointing this to a different location (Llama-Stack Serve) is all we would need to do to move to a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a3474-d8d9-4ea9-a205-61e5a4b46dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "LLAMA_STACK_MODEL=os.getenv(\"LLAMA_STACK_MODEL\")\n",
    "\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_SERVER)\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfc89d-a581-4a0d-91dc-849e9849b822",
   "metadata": {},
   "source": [
    "Now that our client is set up, let's go through some very simple code snippets, to get you familiar with the syntex. If you used other AI Frameworks, this will soon feel very familiar, as Llamastack follows similar principals and terminology, while allowing a standard to help you quickly shift different components in and out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91507f-ed66-49a1-8ed5-281b98c1ea7e",
   "metadata": {},
   "source": [
    "Let's see what vectorDBs our server support out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff36145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get provider list and print it out \n",
    "providers = client.providers.list()\n",
    "for provider in providers:\n",
    "    print(provider)\n",
    "    \n",
    "    \n",
    "# select vector_io providers into array\n",
    "vector_providers = [\n",
    "    provider for provider in client.providers.list() if provider.api == \"vector_io\"\n",
    "]\n",
    "\n",
    "# In this example, we only have one provider, but on other server we might have many. here, we simply select the first one.\n",
    "selected_vector_provider = vector_providers[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ed069",
   "metadata": {},
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=selected_vector_provider.provider_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=selected_vector_provider.provider_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca69a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.types import Document\n",
    "urls = [\n",
    "    \"Crystal_Vortex.md\",\n",
    "    \"Emberwild_Canyon.md\",\n",
    "    \"Frostveil_Tundra.md\",\n",
    "    \"Skyreach_Peaks.md\",\n",
    "    \"Verdant_Mirage.md\",\n",
    "]\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/rhpds/llamastack-lab/refs/heads/main/assets/Parks/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=os.environ['LLAMA_STACK_MODEL'],\n",
    "    instructions=\"You should always use the RAG tool to answer questions, only answer what you are asked, don't add more information than requested\",\n",
    "    tools=[{\n",
    "        \"name\": \"builtin::rag\",\n",
    "        \"args\": {\"vector_db_ids\": [vector_db_id]},\n",
    "    }],\n",
    "       sampling_params={\n",
    "            \"strategy\": {\"type\": \"top_k\", \"temperature\": 0.1, \"top_k\": 2},},\n",
    ")\n",
    "\n",
    "#    instructions=\"You are a helpful assistant that can answer questions about the national parks. Answer only about the park you are asked, don't provide information about other parks. You should always use the RAG tool to answer questions. speak like a sailor\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ce8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's come up with a couple of examples to test the agent\n",
    "examples = [\n",
    "    {\n",
    "        \"input_query\": \"What is the cost of entry to Crystal Vortex\",\n",
    "        \"expected_answer\": \"12$ for individuals and 20 for private car or boat\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What are the Attractions in Frostveil Tundra?\",\n",
    "        \"expected_answer\": \"Northern Lights Viewing Platform,Crystal Snow Elk Observation Trails,Frozen Lake Ice Fishing,Aurora Wolf Tracking Tours\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"when was Verdant Mirage established\",\n",
    "        \"expected_answer\": \"2010\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What are the camping options in Emberwild Canyon\",\n",
    "        \"expected_answer\": \"Canyon Rim Campgrounds,Oasis Camp,Backcountry Camping \"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import pprint\n",
    "import rich\n",
    "rag_agent.sessions=[]\n",
    "for example in examples:\n",
    "    rag_session_id = rag_agent.create_session(session_name=f\"rag_session_{uuid.uuid4()}\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example[\"input_query\"]\n",
    "            }\n",
    "        ],\n",
    "        session_id=rag_session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    rich.print(f\"[bold cyan]Question:[/bold cyan] {example['input_query']}\")\n",
    "    rich.print(f\"[bold yellow]Agent Answer:[/bold yellow] {response.output_message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8af31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session_response = client.agents.session.retrieve(agent_id=rag_agent.agent_id, session_id=rag_agent.sessions[0])\n",
    "pprint(session_response.turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e72c29-2e20-4b01-af9f-a1b221713d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(rag_agent.sessions)\n",
    "session_response = client.agents.session.retrieve(agent_id=rag_agent.agent_id, session_id=rag_agent.sessions[1])\n",
    "pprint(session_response.turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f82d11-e94c-4744-ab90-492e8f04698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unregister all vector databases (THIS IS FOR DEBUG NOT FOR LAB)\n",
    "for vector_db_id in client.vector_dbs.list():\n",
    "    print(f\"Unregistering vector database: {vector_db_id.identifier}\")\n",
    "    client.vector_dbs.unregister(vector_db_id=vector_db_id.identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c114d-2653-419c-820e-060363b2e75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceca93d-beff-4427-8a13-a7c002edbafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70811566-0930-49cd-b5f0-913b49b9f350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d1de6-e3e2-4e0f-aebf-089120bd2391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
