{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f75a76c-cb61-4c32-a334-a5bcd41f4e7d",
   "metadata": {},
   "source": [
    "# Getting Familiar with RAG Basics\n",
    "\n",
    "In this section of the lab, we will review some basic capabilities of RAG, by the end of this section, you will be able to: \n",
    "- Initialize the RAG *agent* to use the LlamaStack server we created in previous sections\n",
    "- Create a simple VectorDB to contain our documents \n",
    "- Embed (insert) our documents into the VectorDB\n",
    "- Retrieve answers based on our documents using the RAG agent and LLM\n",
    "\n",
    "\n",
    "RAG stands for Retrieval-Augmented Generation (RAG). It's a novel approach that combines information retrieval and natural language generation techniques to improve the efficiency of knowledge graph-based systems, such as question-answering models and text summarization. The core idea is to leverage pre-trained language models to retrieve relevant information from a knowledge base or database, and then use this retrieved information to generate high-quality responses. \n",
    "\n",
    "In simple terms, RAG allows us to take documents (PDF, Markdown, Websites or other) that are not availble within our model and allow our agent to provide answers based on the content of those documents. Here are a few examples of what enterprise companies might use RAG: \n",
    "\n",
    "**1. Customer Service Chatbots with RAG**\n",
    "\n",
    "A large retail company could use RAG to power their customer service chatbots. When a customer asks a question about a product, the chatbot uses RAG to retrieve relevant information from its knowledge base and generate a response that is both accurate and helpful. For example, if a customer asks \"What are the features of the new Somephone 13\", the chatbot can use RAG to retrieve information from its database and respond with a detailed list of features, including specifications, pricing, and availability.\n",
    "\n",
    "**2. Personalized Product Recommendations**\n",
    "\n",
    "An e-commerce company like could use RAG to generate personalized product recommendations for customers based on their browsing history and purchase behavior. When a customer visits the website, RAG is used to retrieve information about products that are similar to what they've previously purchased or browsed. The system then generates a list of recommended products, along with detailed descriptions and prices, to help the customer make an informed purchasing decision.\n",
    "\n",
    "**3. Automated Content Generation for Marketing Campaigns**\n",
    "\n",
    "A marketing agency could use RAG to generate high-quality content for their clients' marketing campaigns. For example, if a client wants to create a blog post about the benefits of using artificial intelligence in marketing, RAG can be used to retrieve relevant information from its database and generate a draft of the article. The system can then refine the content based on the client's brand voice and style, ensuring that the final product meets their expectations.\n",
    "\n",
    "**4. Technical Writing Assistance**\n",
    "\n",
    "A software development company could use RAG to assist with technical writing tasks such as generating user manuals, API documentation, and technical guides. When a developer needs to write code comments or documentation for a new feature, RAG can be used to retrieve relevant information from its database and generate high-quality text that is both accurate and concise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089d285-a329-4f61-9edc-3dff39015877",
   "metadata": {},
   "source": [
    "### Install Python Prerequisist\n",
    "\n",
    "As always, let's start by installing the Python Libraries we neeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7268ad4-bda8-427c-9320-3c836e861887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -U llama-stack-client dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62bc6b-c7a9-40a5-93ff-a0b8dfd75851",
   "metadata": {},
   "source": [
    "### Define the LLamastack server and Model\n",
    "\n",
    "Let's point our variables to our Llamastack server and chose our desired model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a729f5-6954-4ffc-844c-5b758204a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for our lab, we will just define our variables manualy here:\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "os.environ['LLAMA_STACK_MODEL'] = 'meta-llama/Llama-3.2-3B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2008b-85be-4be6-9e05-82cc25aac275",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">When running this code in a regular Python application, we would usually like to read environment variables from an `.env` file, for our needs in this lab, we will hard code these in this cell, to make things more clear\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5dad55-f81f-482a-8080-85115078f8b8",
   "metadata": {},
   "source": [
    "### Initialize the *Client* \n",
    "As a first step, let's define our client, provide it our Llama-Stack Server location and select the model we would like to work with, later, we will see that pointing this to a different location (Llama-Stack Serve) is all we would need to do to move to a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a3474-d8d9-4ea9-a205-61e5a4b46dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "LLAMA_STACK_MODEL=os.getenv(\"LLAMA_STACK_MODEL\")\n",
    "\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_SERVER)\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfc89d-a581-4a0d-91dc-849e9849b822",
   "metadata": {},
   "source": [
    "Now that our client is set up, let's go through some very simple code snippets, to get you familiar with the syntex. If you used other AI Frameworks, this will soon feel very familiar, as Llamastack follows similar principals and terminology, while allowing a standard to help you quickly shift different components in and out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91507f-ed66-49a1-8ed5-281b98c1ea7e",
   "metadata": {},
   "source": [
    "### List available vectorDB providers\n",
    "Let's see what vectorDBs our server support out of the box, and select the first available. \n",
    "\n",
    "In Production, we would probably want to select a specific provider, but at this point of our development cycle, we are probably still interested in trying out different VectorDB options, notice that with LlamaStack these are interchangeable and absracted from the code, allowing us to switch them our at will.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff36145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get provider list and print it out \n",
    "\n",
    "print(\"List of providers available in our LlamaStack Server:\")\n",
    "providers = client.providers.list()\n",
    "for provider in providers:\n",
    "    print(provider)\n",
    "    \n",
    "\n",
    "vector_providers = []\n",
    "for provider in client.providers.list():\n",
    "    if provider.api == \"vector_io\":\n",
    "        print(f\"Found VectorDB provider: {provider.provider_id}\\n\")  # Simple print\n",
    "        vector_providers.append(provider)\n",
    "\n",
    "# In this example, we only have one provider, but on other server we might have many. here, we simply select the first one.\n",
    "selected_vector_provider = vector_providers[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ed069",
   "metadata": {},
   "source": [
    "### Register and Initialize a new VectorDB on our LlamaStack Server\n",
    "\n",
    "In this step, you will register a new vector database with the client. This process involves creating a unique identifier for the database and associating it with an embedding model.\n",
    "We will use the built-in \"all-MiniLM-L6-v2\" LLM to embed documetns into our VLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=selected_vector_provider.provider_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de18746-4ff3-4db9-bf56-effa597ac5e2",
   "metadata": {},
   "source": [
    "### Process documents \n",
    "\n",
    "In this step, we will process the documents and embed (insert) them into the vectorDB so we can retrieve them later.\n",
    "Note that we are reading them directly from the web, but we could of course also read them from a local folder. \n",
    "\n",
    "When inserting documents into a VectorDB, documents are split into \"Chunks\". Choices made at this stage can affect results by impacting model accuracy, processing speed, and memory usage. \n",
    "The chunk size can be set by `chunk_size_in_tokens`, which refers to the number of tokens (small units of text) in each processed chunk. \n",
    "\n",
    "In this lab, we will not get into this topic and will just use a simple value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca69a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.types import Document\n",
    "urls = [\n",
    "    \"Crystal_Vortex.md\",\n",
    "    \"Emberwild_Canyon.md\",\n",
    "    \"Frostveil_Tundra.md\",\n",
    "    \"Skyreach_Peaks.md\",\n",
    "    \"Verdant_Mirage.md\",\n",
    "]\n",
    "\n",
    "document_dirctory=\"assets/Badly_Parsed_Parks\"\n",
    "# Read documents into the \"documents\" array\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/rhpds/llamastack-lab/refs/heads/main/{document_dirctory}/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Insert the documents into the vectorDB\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=300,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8424f-45cc-4f14-83a3-61a7fdbaf699",
   "metadata": {},
   "source": [
    "### Define and initialize the Agent\n",
    "\n",
    "In this section, we are creating the *Agent*, defining its *model*, *instructions* (or Prompt), and its *tools*, specifically, the built-in *RAG* tool.\n",
    "\n",
    "Notice that we are passing our vectorDB to the agent using `vector_db_ids` and setting some query configuration options with `query_config`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent\n",
    "\n",
    "query_config = {\n",
    "    \"query_generator_config\": {\n",
    "        \"type\": \"default\",\n",
    "        \"separator\": \" \"\n",
    "    },\n",
    "    \"max_tokens_in_context\": 300,\n",
    "    \"max_chunks\": 2\n",
    "}\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=os.environ['LLAMA_STACK_MODEL'],\n",
    "    instructions=\"You should always use the RAG tool to answer questions, only answer what you are asked, don't add more information than requested\",\n",
    "    tools=[{\n",
    "        \"name\": \"builtin::rag\",\n",
    "        \"args\": {\"vector_db_ids\": [vector_db_id],\"query_config\": query_config  },\n",
    "    }],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3c0c3-c9ea-40ca-9b06-7ab9d1bcbd82",
   "metadata": {},
   "source": [
    "### Create a list of questions to test our retrieval agent\n",
    "\n",
    "We will create an array of questions so we can test our retrieval agent.\n",
    "\n",
    "You will notice, that for each example we also provided the expected answer. In a real-world scenario, we would use the answers to score and evaluate the responses. \n",
    "This is a crucial part of development if this kind of function, Llamastack offers build-in to manage exactly this type of process, allowing you to test many models, methods, VectorDBs and having the metrics to see which one works the best and to allow you to see if your implementation drifted over time. \n",
    "\n",
    "Similar to regresssion testing in traditional code scenarios, consider a company that wants to evaluate a different model and needs a way to measure the improvement/degradation in quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ce8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's come up with a couple of examples to test the agent\n",
    "examples = [\n",
    "    {\n",
    "        \"input_query\": \"What is the cost of entry to Crystal Vortex\",\n",
    "        \"expected_answer\": \"12$ for individuals and 20 for private car or boat\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What are the Attractions in Frostveil Tundra?\",\n",
    "        \"expected_answer\": \"Northern Lights Viewing Platform,Crystal Snow Elk Observation Trails,Frozen Lake Ice Fishing,Aurora Wolf Tracking Tours\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"when was Verdant Mirage established\",\n",
    "        \"expected_answer\": \"2010\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What are the camping options in Emberwild Canyon\",\n",
    "        \"expected_answer\": \"Canyon Rim Campgrounds,Oasis Camp,Backcountry Camping \"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78588c73-b52d-45e4-a295-e6fcc6d0e5ed",
   "metadata": {},
   "source": [
    "### Run Retrieval agent\n",
    "\n",
    "This step will get 4 separate responses from our agent, allowing us to manually evaluate its capabilities. \n",
    "> **Note:**\n",
    "> You might have a quick laugh as the initial results will be hit and miss. This is an initial implementation and tuning, scoring, and tuning will be the next steps in a real-world scenario.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import pprint\n",
    "import rich\n",
    "\n",
    "rag_agent.sessions=[]\n",
    "for example in examples:\n",
    "    rag_session_id = rag_agent.create_session(session_name=f\"rag_session_{uuid.uuid4()}\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example[\"input_query\"]\n",
    "            }\n",
    "        ],\n",
    "        session_id=rag_session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    rich.print(f\"[bold cyan]Question:[/bold cyan] {example['input_query']}\")\n",
    "    rich.print(f\"[bold yellow]Agent Answer:[/bold yellow] {response.output_message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e407059-c40a-462f-a276-a7947f7f317c",
   "metadata": {},
   "source": [
    "### Inspecting the Agent's process\n",
    "\n",
    "If you are interested, you can review the steps the agent has taken and see which documents were retrieved. \n",
    "This is a crucial debugging tool when trying to understand what is causing your retrieval to succeed or fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8af31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Session ID\\t\\t Question\")\n",
    "i=0\n",
    "for session in rag_agent.sessions:\n",
    "    session_response = client.agents.session.retrieve(agent_id=rag_agent.agent_id, session_id=rag_agent.sessions[i])\n",
    "    print(i,\"\\t\\t\\t\",session_response.turns[0].input_messages[0])\n",
    "    i=i+1    \n",
    "\n",
    "## Set this to whichever session you want to review:\n",
    "session_to_debug=0\n",
    "\n",
    "session_response = client.agents.session.retrieve(agent_id=rag_agent.agent_id, session_id=rag_agent.sessions[session_to_debug])\n",
    "pprint(session_response.turns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52230bb1-ba96-4973-8195-ecae9d796437",
   "metadata": {},
   "source": [
    "## Can we do a little better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d96e1a-f2d4-4cd3-a3fb-483c6ca7b6ce",
   "metadata": {},
   "source": [
    "### Erase existing VectorDBs\n",
    "\n",
    "If you want to play with some chunk options and see if you can improve the results, you might want to delete your VectorDBs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d529c6-cd9f-49c1-928d-afa2f073b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unregister all vector databases (THIS IS FOR DEBUG NOT FOR LAB)\n",
    "for vector_db_id in client.vector_dbs.list():\n",
    "    print(f\"Unregistering vector database: {vector_db_id.identifier}\")\n",
    "    client.vector_dbs.unregister(vector_db_id=vector_db_id.identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d1de6-e3e2-4e0f-aebf-089120bd2391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
